{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from VisionUtils import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectFpr, mutual_info_classif\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat1 = np.load(\"features/feat1.npy\")\n",
    "feat2 = np.load(\"features/feat2.npy\")\n",
    "labels = np.load(\"features/labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(feat1, feat2):\n",
    "    f1 = [feat1[i1].reshape(-1,1) for i1 in range(len(feat1))]\n",
    "    f2 = [feat2[i2].reshape(-1,1) for i2 in range(len(feat2))]\n",
    "    cos_d = np.array([feat_distance_cosine_scalar(f1[i].T, f2[i]) for i in range(len(feat1))])\n",
    "    cos_d = cos_d.reshape(-1,1)\n",
    "    sqr_diff = np.power(np.abs(feat1- feat2), 2)\n",
    "    rat = feat1/feat2\n",
    "    data = np.hstack([cos_d, sqr_diff])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = trans(feat1, feat2)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(scaled_data, labels, \n",
    "                                                shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "he_init = tf.keras.initializers.VarianceScaling()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense( 128, activation = 'relu', kernel_initializer = he_init, \n",
    "                kernel_regularizer = l1(0.001), \n",
    "                #input_shape = (1,128)\n",
    "               ))\n",
    "model.add(Dense(32, activation = 'relu',kernel_initializer = he_init))\n",
    "model.add(Dense(1, activation = \"tanh\", kernel_initializer = he_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "mcp = ModelCheckpoint(\"models_chpt/best_modelD.hdf5\",verbose = 1, \n",
    "                      monitor = \"val_loss\", save_best_only = True, save_weights_only = True)\n",
    "red_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor =0.5)\n",
    "opt = tf.keras.optimizers.Adam(0.01)\n",
    "model.compile(optimizer = opt, loss = \"binary_crossentropy\", \n",
    "              metrics = [\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21450 samples, validate on 7150 samples\n",
      "Epoch 1/400\n",
      " 1000/21450 [>.............................] - ETA: 1s - loss: 2.9236 - acc: 0.4700\n",
      "Epoch 00001: val_loss improved from inf to 0.84806, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 13us/sample - loss: 1.4512 - acc: 0.6972 - val_loss: 0.8481 - val_acc: 0.8916\n",
      "Epoch 2/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.8137 - acc: 0.8890\n",
      "Epoch 00002: val_loss improved from 0.84806 to 0.61122, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.7517 - acc: 0.8704 - val_loss: 0.6112 - val_acc: 0.8926\n",
      "Epoch 3/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.6064 - acc: 0.8810\n",
      "Epoch 00003: val_loss improved from 0.61122 to 0.44603, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.5240 - acc: 0.8857 - val_loss: 0.4460 - val_acc: 0.8884\n",
      "Epoch 4/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.4387 - acc: 0.8860\n",
      "Epoch 00004: val_loss improved from 0.44603 to 0.37364, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 4us/sample - loss: 0.4085 - acc: 0.8898 - val_loss: 0.3736 - val_acc: 0.8980\n",
      "Epoch 5/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3420 - acc: 0.8950\n",
      "Epoch 00005: val_loss improved from 0.37364 to 0.35238, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3861 - acc: 0.8876 - val_loss: 0.3524 - val_acc: 0.8966\n",
      "Epoch 6/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3221 - acc: 0.8910\n",
      "Epoch 00006: val_loss did not improve from 0.35238\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3450 - acc: 0.8902 - val_loss: 0.4351 - val_acc: 0.8385\n",
      "Epoch 7/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.4561 - acc: 0.8290\n",
      "Epoch 00007: val_loss did not improve from 0.35238\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.4335 - acc: 0.8704 - val_loss: 0.4162 - val_acc: 0.8846\n",
      "Epoch 8/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3633 - acc: 0.8970\n",
      "Epoch 00008: val_loss did not improve from 0.35238\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3901 - acc: 0.8809 - val_loss: 0.3550 - val_acc: 0.8920\n",
      "Epoch 9/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3696 - acc: 0.8860\n",
      "Epoch 00009: val_loss improved from 0.35238 to 0.31957, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3525 - acc: 0.8895 - val_loss: 0.3196 - val_acc: 0.8964\n",
      "Epoch 10/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3343 - acc: 0.8930\n",
      "Epoch 00010: val_loss improved from 0.31957 to 0.31787, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3281 - acc: 0.8912 - val_loss: 0.3179 - val_acc: 0.8944\n",
      "Epoch 11/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3548 - acc: 0.8840\n",
      "Epoch 00011: val_loss improved from 0.31787 to 0.30521, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3232 - acc: 0.8911 - val_loss: 0.3052 - val_acc: 0.8957\n",
      "Epoch 12/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2685 - acc: 0.9130\n",
      "Epoch 00012: val_loss improved from 0.30521 to 0.30118, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3126 - acc: 0.8919 - val_loss: 0.3012 - val_acc: 0.8957\n",
      "Epoch 13/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3433 - acc: 0.8890\n",
      "Epoch 00013: val_loss improved from 0.30118 to 0.29745, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3156 - acc: 0.8898 - val_loss: 0.2974 - val_acc: 0.8945\n",
      "Epoch 14/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3106 - acc: 0.8940\n",
      "Epoch 00014: val_loss did not improve from 0.29745\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3097 - acc: 0.8903 - val_loss: 0.2983 - val_acc: 0.8961\n",
      "Epoch 15/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3063 - acc: 0.8970\n",
      "Epoch 00015: val_loss did not improve from 0.29745\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3285 - acc: 0.8866 - val_loss: 0.3446 - val_acc: 0.8926\n",
      "Epoch 16/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3367 - acc: 0.8940\n",
      "Epoch 00016: val_loss did not improve from 0.29745\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3327 - acc: 0.8895 - val_loss: 0.3043 - val_acc: 0.8961\n",
      "Epoch 17/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2957 - acc: 0.8850\n",
      "Epoch 00017: val_loss improved from 0.29745 to 0.29075, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3111 - acc: 0.8924 - val_loss: 0.2907 - val_acc: 0.8948\n",
      "Epoch 18/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3368 - acc: 0.8870\n",
      "Epoch 00018: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3371 - acc: 0.8875 - val_loss: 0.5446 - val_acc: 0.8274\n",
      "Epoch 19/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.5541 - acc: 0.8170\n",
      "Epoch 00019: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.5226 - acc: 0.8552 - val_loss: 0.5237 - val_acc: 0.8909\n",
      "Epoch 20/400\n",
      "20000/21450 [==========================>...] - ETA: 0s - loss: 0.5112 - acc: 0.8688\n",
      "Epoch 00020: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.5089 - acc: 0.8700 - val_loss: 0.4429 - val_acc: 0.8910\n",
      "Epoch 21/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.4578 - acc: 0.8780\n",
      "Epoch 00021: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.4119 - acc: 0.8872 - val_loss: 0.3809 - val_acc: 0.8895\n",
      "Epoch 22/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3769 - acc: 0.8780\n",
      "Epoch 00022: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.4061 - acc: 0.8795 - val_loss: 0.3858 - val_acc: 0.8873\n",
      "Epoch 23/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3636 - acc: 0.8980\n",
      "Epoch 00023: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.6795 - acc: 0.7500 - val_loss: 0.6739 - val_acc: 0.7934\n",
      "Epoch 24/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.6579 - acc: 0.7970\n",
      "Epoch 00024: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.6021 - acc: 0.8244 - val_loss: 0.5849 - val_acc: 0.8716\n",
      "Epoch 25/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.5773 - acc: 0.8760\n",
      "Epoch 00025: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.5494 - acc: 0.8768 - val_loss: 0.5303 - val_acc: 0.8829\n",
      "Epoch 26/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.4840 - acc: 0.9000\n",
      "Epoch 00026: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.4990 - acc: 0.8829 - val_loss: 0.4851 - val_acc: 0.8880\n",
      "Epoch 27/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.4207 - acc: 0.9030\n",
      "Epoch 00027: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.4415 - acc: 0.8849 - val_loss: 0.4578 - val_acc: 0.8671\n",
      "Epoch 28/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.4810 - acc: 0.8590\n",
      "Epoch 00028: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.4194 - acc: 0.8860 - val_loss: 0.4012 - val_acc: 0.8910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3691 - acc: 0.9000\n",
      "Epoch 00029: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3918 - acc: 0.8887 - val_loss: 0.3961 - val_acc: 0.8909\n",
      "Epoch 30/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3559 - acc: 0.8960\n",
      "Epoch 00030: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3832 - acc: 0.8881 - val_loss: 0.3877 - val_acc: 0.8903\n",
      "Epoch 31/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.4099 - acc: 0.8720\n",
      "Epoch 00031: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3797 - acc: 0.8869 - val_loss: 0.3777 - val_acc: 0.8876\n",
      "Epoch 32/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3847 - acc: 0.8840\n",
      "Epoch 00032: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3701 - acc: 0.8876 - val_loss: 0.3743 - val_acc: 0.8919\n",
      "Epoch 33/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3844 - acc: 0.8840\n",
      "Epoch 00033: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3629 - acc: 0.8893 - val_loss: 0.3628 - val_acc: 0.8877\n",
      "Epoch 34/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3590 - acc: 0.8860\n",
      "Epoch 00034: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3590 - acc: 0.8889 - val_loss: 0.3595 - val_acc: 0.8927\n",
      "Epoch 35/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3270 - acc: 0.8990\n",
      "Epoch 00035: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3509 - acc: 0.8909 - val_loss: 0.3633 - val_acc: 0.8937\n",
      "Epoch 36/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3275 - acc: 0.8880\n",
      "Epoch 00036: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3598 - acc: 0.8859 - val_loss: 0.3576 - val_acc: 0.8954\n",
      "Epoch 37/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3141 - acc: 0.9050\n",
      "Epoch 00037: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3513 - acc: 0.8912 - val_loss: 0.3511 - val_acc: 0.8937\n",
      "Epoch 38/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3453 - acc: 0.8990\n",
      "Epoch 00038: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3397 - acc: 0.8923 - val_loss: 0.3491 - val_acc: 0.8961\n",
      "Epoch 39/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3314 - acc: 0.8970\n",
      "Epoch 00039: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3341 - acc: 0.8920 - val_loss: 0.3503 - val_acc: 0.8954\n",
      "Epoch 40/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3498 - acc: 0.9050\n",
      "Epoch 00040: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3297 - acc: 0.8926 - val_loss: 0.3477 - val_acc: 0.8969\n",
      "Epoch 41/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3168 - acc: 0.8940\n",
      "Epoch 00041: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3269 - acc: 0.8923 - val_loss: 0.3436 - val_acc: 0.8962\n",
      "Epoch 42/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3359 - acc: 0.8810\n",
      "Epoch 00042: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3247 - acc: 0.8922 - val_loss: 0.3401 - val_acc: 0.8978\n",
      "Epoch 43/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3038 - acc: 0.8960\n",
      "Epoch 00043: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3222 - acc: 0.8925 - val_loss: 0.3328 - val_acc: 0.8978\n",
      "Epoch 44/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3345 - acc: 0.8990\n",
      "Epoch 00044: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3233 - acc: 0.8921 - val_loss: 0.3468 - val_acc: 0.8961\n",
      "Epoch 45/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3530 - acc: 0.8810\n",
      "Epoch 00045: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3231 - acc: 0.8938 - val_loss: 0.3312 - val_acc: 0.8961\n",
      "Epoch 46/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3145 - acc: 0.8920\n",
      "Epoch 00046: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3199 - acc: 0.8939 - val_loss: 0.3325 - val_acc: 0.8950\n",
      "Epoch 47/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3262 - acc: 0.9030\n",
      "Epoch 00047: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3167 - acc: 0.8938 - val_loss: 0.3294 - val_acc: 0.8958\n",
      "Epoch 48/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3119 - acc: 0.8980\n",
      "Epoch 00048: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3138 - acc: 0.8934 - val_loss: 0.3244 - val_acc: 0.8958\n",
      "Epoch 49/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2839 - acc: 0.9160\n",
      "Epoch 00049: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3122 - acc: 0.8939 - val_loss: 0.3243 - val_acc: 0.8959\n",
      "Epoch 50/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2977 - acc: 0.9040\n",
      "Epoch 00050: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3115 - acc: 0.8941 - val_loss: 0.3251 - val_acc: 0.8954\n",
      "Epoch 51/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3332 - acc: 0.8830\n",
      "Epoch 00051: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3106 - acc: 0.8938 - val_loss: 0.3205 - val_acc: 0.8966\n",
      "Epoch 52/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3183 - acc: 0.8950\n",
      "Epoch 00052: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3100 - acc: 0.8940 - val_loss: 0.3267 - val_acc: 0.8950\n",
      "Epoch 53/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2657 - acc: 0.9130\n",
      "Epoch 00053: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3094 - acc: 0.8940 - val_loss: 0.3203 - val_acc: 0.8965\n",
      "Epoch 54/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3608 - acc: 0.8810\n",
      "Epoch 00054: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3085 - acc: 0.8938 - val_loss: 0.3196 - val_acc: 0.8961\n",
      "Epoch 55/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2944 - acc: 0.8950\n",
      "Epoch 00055: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3077 - acc: 0.8939 - val_loss: 0.3187 - val_acc: 0.8959\n",
      "Epoch 56/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3184 - acc: 0.8870\n",
      "Epoch 00056: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3070 - acc: 0.8938 - val_loss: 0.3182 - val_acc: 0.8962\n",
      "Epoch 57/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3035 - acc: 0.8970\n",
      "Epoch 00057: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3068 - acc: 0.8938 - val_loss: 0.3174 - val_acc: 0.8958\n",
      "Epoch 58/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2793 - acc: 0.9120\n",
      "Epoch 00058: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3053 - acc: 0.8941 - val_loss: 0.3161 - val_acc: 0.8962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3545 - acc: 0.8830\n",
      "Epoch 00059: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3046 - acc: 0.8940 - val_loss: 0.3156 - val_acc: 0.8958\n",
      "Epoch 60/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2814 - acc: 0.8980\n",
      "Epoch 00060: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3043 - acc: 0.8941 - val_loss: 0.3155 - val_acc: 0.8962\n",
      "Epoch 61/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2797 - acc: 0.9040\n",
      "Epoch 00061: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3039 - acc: 0.8942 - val_loss: 0.3150 - val_acc: 0.8958\n",
      "Epoch 62/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2893 - acc: 0.8980\n",
      "Epoch 00062: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3035 - acc: 0.8943 - val_loss: 0.3146 - val_acc: 0.8959\n",
      "Epoch 63/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3230 - acc: 0.8850\n",
      "Epoch 00063: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3032 - acc: 0.8942 - val_loss: 0.3143 - val_acc: 0.8958\n",
      "Epoch 64/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3218 - acc: 0.8910\n",
      "Epoch 00064: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3029 - acc: 0.8942 - val_loss: 0.3139 - val_acc: 0.8958\n",
      "Epoch 65/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3054 - acc: 0.8950\n",
      "Epoch 00065: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3026 - acc: 0.8943 - val_loss: 0.3137 - val_acc: 0.8965\n",
      "Epoch 66/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2860 - acc: 0.8870\n",
      "Epoch 00066: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3023 - acc: 0.8942 - val_loss: 0.3135 - val_acc: 0.8964\n",
      "Epoch 67/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2896 - acc: 0.8970\n",
      "Epoch 00067: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3021 - acc: 0.8944 - val_loss: 0.3119 - val_acc: 0.8961\n",
      "Epoch 68/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2964 - acc: 0.8970\n",
      "Epoch 00068: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3014 - acc: 0.8942 - val_loss: 0.3124 - val_acc: 0.8961\n",
      "Epoch 69/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2821 - acc: 0.9090\n",
      "Epoch 00069: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3011 - acc: 0.8943 - val_loss: 0.3122 - val_acc: 0.8962\n",
      "Epoch 70/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3049 - acc: 0.8810\n",
      "Epoch 00070: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3009 - acc: 0.8944 - val_loss: 0.3120 - val_acc: 0.8962\n",
      "Epoch 71/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2621 - acc: 0.9050\n",
      "Epoch 00071: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3008 - acc: 0.8943 - val_loss: 0.3119 - val_acc: 0.8964\n",
      "Epoch 72/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2962 - acc: 0.9040\n",
      "Epoch 00072: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3006 - acc: 0.8940 - val_loss: 0.3117 - val_acc: 0.8961\n",
      "Epoch 73/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3670 - acc: 0.8870\n",
      "Epoch 00073: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3004 - acc: 0.8940 - val_loss: 0.3116 - val_acc: 0.8962\n",
      "Epoch 74/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2833 - acc: 0.9010\n",
      "Epoch 00074: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3003 - acc: 0.8939 - val_loss: 0.3114 - val_acc: 0.8962\n",
      "Epoch 75/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3042 - acc: 0.8820\n",
      "Epoch 00075: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3001 - acc: 0.8941 - val_loss: 0.3112 - val_acc: 0.8962\n",
      "Epoch 76/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3609 - acc: 0.8740\n",
      "Epoch 00076: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3000 - acc: 0.8942 - val_loss: 0.3111 - val_acc: 0.8964\n",
      "Epoch 77/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2964 - acc: 0.9130\n",
      "Epoch 00077: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2998 - acc: 0.8940 - val_loss: 0.3110 - val_acc: 0.8958\n",
      "Epoch 78/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3016 - acc: 0.8970\n",
      "Epoch 00078: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2995 - acc: 0.8944 - val_loss: 0.3106 - val_acc: 0.8961\n",
      "Epoch 79/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3285 - acc: 0.9010\n",
      "Epoch 00079: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2993 - acc: 0.8942 - val_loss: 0.3105 - val_acc: 0.8959\n",
      "Epoch 80/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2936 - acc: 0.8960\n",
      "Epoch 00080: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2993 - acc: 0.8943 - val_loss: 0.3105 - val_acc: 0.8964\n",
      "Epoch 81/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3088 - acc: 0.8970\n",
      "Epoch 00081: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2991 - acc: 0.8942 - val_loss: 0.3103 - val_acc: 0.8958\n",
      "Epoch 82/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3218 - acc: 0.8840\n",
      "Epoch 00082: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2991 - acc: 0.8943 - val_loss: 0.3102 - val_acc: 0.8962\n",
      "Epoch 83/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2981 - acc: 0.9010\n",
      "Epoch 00083: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2990 - acc: 0.8945 - val_loss: 0.3101 - val_acc: 0.8955\n",
      "Epoch 84/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2850 - acc: 0.8980\n",
      "Epoch 00084: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2989 - acc: 0.8943 - val_loss: 0.3101 - val_acc: 0.8955\n",
      "Epoch 85/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2876 - acc: 0.8930\n",
      "Epoch 00085: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2988 - acc: 0.8942 - val_loss: 0.3100 - val_acc: 0.8955\n",
      "Epoch 86/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2862 - acc: 0.8920\n",
      "Epoch 00086: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2987 - acc: 0.8943 - val_loss: 0.3099 - val_acc: 0.8959\n",
      "Epoch 87/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3180 - acc: 0.8910\n",
      "Epoch 00087: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2987 - acc: 0.8944 - val_loss: 0.3098 - val_acc: 0.8955\n",
      "Epoch 88/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3363 - acc: 0.8770\n",
      "Epoch 00088: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2985 - acc: 0.8943 - val_loss: 0.3096 - val_acc: 0.8957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2955 - acc: 0.8960\n",
      "Epoch 00089: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2984 - acc: 0.8944 - val_loss: 0.3096 - val_acc: 0.8958\n",
      "Epoch 90/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2792 - acc: 0.9010\n",
      "Epoch 00090: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2984 - acc: 0.8944 - val_loss: 0.3096 - val_acc: 0.8959\n",
      "Epoch 91/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3303 - acc: 0.8950\n",
      "Epoch 00091: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2983 - acc: 0.8945 - val_loss: 0.3095 - val_acc: 0.8958\n",
      "Epoch 92/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3269 - acc: 0.8890\n",
      "Epoch 00092: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2983 - acc: 0.8942 - val_loss: 0.3095 - val_acc: 0.8958\n",
      "Epoch 93/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3232 - acc: 0.8770\n",
      "Epoch 00093: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2982 - acc: 0.8943 - val_loss: 0.3094 - val_acc: 0.8957\n",
      "Epoch 94/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2910 - acc: 0.8970\n",
      "Epoch 00094: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2982 - acc: 0.8946 - val_loss: 0.3094 - val_acc: 0.8959\n",
      "Epoch 95/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3047 - acc: 0.8900\n",
      "Epoch 00095: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2982 - acc: 0.8947 - val_loss: 0.3093 - val_acc: 0.8959\n",
      "Epoch 96/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2626 - acc: 0.9100\n",
      "Epoch 00096: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2981 - acc: 0.8945 - val_loss: 0.3093 - val_acc: 0.8959\n",
      "Epoch 97/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3096 - acc: 0.8890\n",
      "Epoch 00097: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2981 - acc: 0.8945 - val_loss: 0.3092 - val_acc: 0.8955\n",
      "Epoch 98/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3022 - acc: 0.9010\n",
      "Epoch 00098: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2980 - acc: 0.8946 - val_loss: 0.3092 - val_acc: 0.8959\n",
      "Epoch 99/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2897 - acc: 0.8890\n",
      "Epoch 00099: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2979 - acc: 0.8946 - val_loss: 0.3091 - val_acc: 0.8959\n",
      "Epoch 100/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3178 - acc: 0.8870\n",
      "Epoch 00100: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2979 - acc: 0.8945 - val_loss: 0.3091 - val_acc: 0.8959\n",
      "Epoch 101/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2716 - acc: 0.9050\n",
      "Epoch 00101: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2979 - acc: 0.8946 - val_loss: 0.3091 - val_acc: 0.8958\n",
      "Epoch 102/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2834 - acc: 0.9140\n",
      "Epoch 00102: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2979 - acc: 0.8946 - val_loss: 0.3091 - val_acc: 0.8958\n",
      "Epoch 103/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3560 - acc: 0.8890\n",
      "Epoch 00103: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2978 - acc: 0.8946 - val_loss: 0.3090 - val_acc: 0.8958\n",
      "Epoch 104/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2889 - acc: 0.9000\n",
      "Epoch 00104: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2978 - acc: 0.8945 - val_loss: 0.3090 - val_acc: 0.8958\n",
      "Epoch 105/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3391 - acc: 0.8880\n",
      "Epoch 00105: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2978 - acc: 0.8945 - val_loss: 0.3090 - val_acc: 0.8961\n",
      "Epoch 106/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3146 - acc: 0.8930\n",
      "Epoch 00106: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2978 - acc: 0.8944 - val_loss: 0.3090 - val_acc: 0.8961\n",
      "Epoch 107/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2992 - acc: 0.8930\n",
      "Epoch 00107: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2978 - acc: 0.8946 - val_loss: 0.3090 - val_acc: 0.8961\n",
      "Epoch 108/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2652 - acc: 0.9130\n",
      "Epoch 00108: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2977 - acc: 0.8945 - val_loss: 0.3089 - val_acc: 0.8961\n",
      "Epoch 109/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2852 - acc: 0.9000\n",
      "Epoch 00109: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2977 - acc: 0.8945 - val_loss: 0.3089 - val_acc: 0.8961\n",
      "Epoch 110/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2549 - acc: 0.9090\n",
      "Epoch 00110: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2977 - acc: 0.8945 - val_loss: 0.3089 - val_acc: 0.8961\n",
      "Epoch 111/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2743 - acc: 0.9010\n",
      "Epoch 00111: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2977 - acc: 0.8945 - val_loss: 0.3089 - val_acc: 0.8961\n",
      "Epoch 112/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3007 - acc: 0.8980\n",
      "Epoch 00112: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2976 - acc: 0.8945 - val_loss: 0.3089 - val_acc: 0.8961\n",
      "Epoch 113/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2803 - acc: 0.8910\n",
      "Epoch 00113: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2976 - acc: 0.8945 - val_loss: 0.3089 - val_acc: 0.8961\n",
      "Epoch 114/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3010 - acc: 0.8930\n",
      "Epoch 00114: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2976 - acc: 0.8945 - val_loss: 0.3088 - val_acc: 0.8961\n",
      "Epoch 115/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2731 - acc: 0.9060\n",
      "Epoch 00115: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2976 - acc: 0.8945 - val_loss: 0.3088 - val_acc: 0.8961\n",
      "Epoch 116/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3031 - acc: 0.8900\n",
      "Epoch 00116: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2976 - acc: 0.8946 - val_loss: 0.3088 - val_acc: 0.8961\n",
      "Epoch 117/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3335 - acc: 0.8710\n",
      "Epoch 00117: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2976 - acc: 0.8945 - val_loss: 0.3088 - val_acc: 0.8961\n",
      "Epoch 118/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2743 - acc: 0.9010\n",
      "Epoch 00118: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2976 - acc: 0.8946 - val_loss: 0.3088 - val_acc: 0.8961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2936 - acc: 0.9000\n",
      "Epoch 00119: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2975 - acc: 0.8945 - val_loss: 0.3088 - val_acc: 0.8961\n",
      "Epoch 120/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2956 - acc: 0.8910\n",
      "Epoch 00120: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2975 - acc: 0.8945 - val_loss: 0.3088 - val_acc: 0.8961\n",
      "Epoch 121/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2706 - acc: 0.8970\n",
      "Epoch 00121: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2975 - acc: 0.8945 - val_loss: 0.3088 - val_acc: 0.8961\n",
      "Epoch 122/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2989 - acc: 0.8980\n",
      "Epoch 00122: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2975 - acc: 0.8946 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 123/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2894 - acc: 0.9100\n",
      "Epoch 00123: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2975 - acc: 0.8946 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 124/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2985 - acc: 0.8840\n",
      "Epoch 00124: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2975 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 125/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2902 - acc: 0.8920\n",
      "Epoch 00125: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2975 - acc: 0.8946 - val_loss: 0.3087 - val_acc: 0.8959\n",
      "Epoch 126/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3398 - acc: 0.8920\n",
      "Epoch 00126: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2975 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 127/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3196 - acc: 0.8950\n",
      "Epoch 00127: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2975 - acc: 0.8946 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 128/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2909 - acc: 0.8880\n",
      "Epoch 00128: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2975 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 129/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3129 - acc: 0.8790\n",
      "Epoch 00129: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2975 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 130/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3242 - acc: 0.8870\n",
      "Epoch 00130: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2975 - acc: 0.8946 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 131/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2858 - acc: 0.9060\n",
      "Epoch 00131: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2975 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 132/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3060 - acc: 0.8770\n",
      "Epoch 00132: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2975 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 133/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2943 - acc: 0.8940\n",
      "Epoch 00133: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2975 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 134/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3480 - acc: 0.8650\n",
      "Epoch 00134: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2975 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 135/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3334 - acc: 0.8900\n",
      "Epoch 00135: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2975 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 136/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2849 - acc: 0.9010\n",
      "Epoch 00136: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8946 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 137/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2971 - acc: 0.8920\n",
      "Epoch 00137: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 138/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2996 - acc: 0.9000\n",
      "Epoch 00138: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 139/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3029 - acc: 0.8930\n",
      "Epoch 00139: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 140/400\n",
      "20000/21450 [==========================>...] - ETA: 0s - loss: 0.2973 - acc: 0.8949\n",
      "Epoch 00140: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 141/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3195 - acc: 0.8960\n",
      "Epoch 00141: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 142/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2953 - acc: 0.8810\n",
      "Epoch 00142: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 143/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3378 - acc: 0.8790\n",
      "Epoch 00143: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 144/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2707 - acc: 0.9080\n",
      "Epoch 00144: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 145/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3178 - acc: 0.8980\n",
      "Epoch 00145: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 146/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3264 - acc: 0.8820\n",
      "Epoch 00146: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 147/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2788 - acc: 0.8960\n",
      "Epoch 00147: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 148/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3194 - acc: 0.8860\n",
      "Epoch 00148: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3173 - acc: 0.8760\n",
      "Epoch 00149: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 150/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3259 - acc: 0.8850\n",
      "Epoch 00150: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3087 - val_acc: 0.8961\n",
      "Epoch 151/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3168 - acc: 0.8870\n",
      "Epoch 00151: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 152/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2735 - acc: 0.9000\n",
      "Epoch 00152: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 153/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2790 - acc: 0.8950\n",
      "Epoch 00153: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 154/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3037 - acc: 0.9040\n",
      "Epoch 00154: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 155/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3061 - acc: 0.8950\n",
      "Epoch 00155: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 156/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2773 - acc: 0.9000\n",
      "Epoch 00156: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 157/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2766 - acc: 0.9010\n",
      "Epoch 00157: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 158/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2835 - acc: 0.9100\n",
      "Epoch 00158: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 159/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2796 - acc: 0.8850\n",
      "Epoch 00159: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 160/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2950 - acc: 0.9030\n",
      "Epoch 00160: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 161/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2937 - acc: 0.9010\n",
      "Epoch 00161: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 162/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2975 - acc: 0.8850\n",
      "Epoch 00162: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 163/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2881 - acc: 0.8820\n",
      "Epoch 00163: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 164/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2621 - acc: 0.9080\n",
      "Epoch 00164: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 165/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2918 - acc: 0.8870\n",
      "Epoch 00165: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 166/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3617 - acc: 0.8900\n",
      "Epoch 00166: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 167/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2979 - acc: 0.8920\n",
      "Epoch 00167: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 168/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2909 - acc: 0.9000\n",
      "Epoch 00168: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 169/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2754 - acc: 0.9070\n",
      "Epoch 00169: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 170/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3350 - acc: 0.8900\n",
      "Epoch 00170: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 171/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2469 - acc: 0.9150\n",
      "Epoch 00171: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 172/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3110 - acc: 0.9030\n",
      "Epoch 00172: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 173/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3329 - acc: 0.8930\n",
      "Epoch 00173: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 174/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2965 - acc: 0.8980\n",
      "Epoch 00174: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 175/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3295 - acc: 0.8770\n",
      "Epoch 00175: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 176/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2952 - acc: 0.8920\n",
      "Epoch 00176: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 177/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2931 - acc: 0.9030\n",
      "Epoch 00177: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 178/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2848 - acc: 0.9060\n",
      "Epoch 00178: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2952 - acc: 0.9060\n",
      "Epoch 00179: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 180/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2563 - acc: 0.9110\n",
      "Epoch 00180: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 181/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2808 - acc: 0.8910\n",
      "Epoch 00181: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 182/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2954 - acc: 0.8950\n",
      "Epoch 00182: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 183/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2983 - acc: 0.8930\n",
      "Epoch 00183: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 184/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3140 - acc: 0.8830\n",
      "Epoch 00184: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 185/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2770 - acc: 0.8940\n",
      "Epoch 00185: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 186/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2896 - acc: 0.9180\n",
      "Epoch 00186: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 187/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2936 - acc: 0.8920\n",
      "Epoch 00187: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 188/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2863 - acc: 0.8920\n",
      "Epoch 00188: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 189/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3199 - acc: 0.8960\n",
      "Epoch 00189: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 190/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3022 - acc: 0.8900\n",
      "Epoch 00190: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 191/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2978 - acc: 0.8980\n",
      "Epoch 00191: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 192/400\n",
      "21000/21450 [============================>.] - ETA: 0s - loss: 0.2977 - acc: 0.8944\n",
      "Epoch 00192: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 193/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3184 - acc: 0.8760\n",
      "Epoch 00193: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 194/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3232 - acc: 0.8960\n",
      "Epoch 00194: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 195/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2831 - acc: 0.9040\n",
      "Epoch 00195: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 196/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2912 - acc: 0.8990\n",
      "Epoch 00196: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 197/400\n",
      "21000/21450 [============================>.] - ETA: 0s - loss: 0.2971 - acc: 0.8946\n",
      "Epoch 00197: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 198/400\n",
      "19000/21450 [=========================>....] - ETA: 0s - loss: 0.2954 - acc: 0.8947\n",
      "Epoch 00198: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 4us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 199/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2914 - acc: 0.9100\n",
      "Epoch 00199: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 200/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2736 - acc: 0.9160\n",
      "Epoch 00200: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 201/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2998 - acc: 0.8980\n",
      "Epoch 00201: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 202/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2457 - acc: 0.9080\n",
      "Epoch 00202: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 203/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2930 - acc: 0.9010\n",
      "Epoch 00203: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 204/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2993 - acc: 0.8930\n",
      "Epoch 00204: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 205/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2959 - acc: 0.9000\n",
      "Epoch 00205: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 206/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3143 - acc: 0.8990\n",
      "Epoch 00206: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 207/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3427 - acc: 0.8710\n",
      "Epoch 00207: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 208/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2623 - acc: 0.9080\n",
      "Epoch 00208: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 209/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3007 - acc: 0.8820\n",
      "Epoch 00209: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 210/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3007 - acc: 0.8930\n",
      "Epoch 00210: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 211/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2623 - acc: 0.9200\n",
      "Epoch 00211: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 212/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3094 - acc: 0.8760\n",
      "Epoch 00212: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 213/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2950 - acc: 0.8890\n",
      "Epoch 00213: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 214/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2804 - acc: 0.8950\n",
      "Epoch 00214: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 215/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2905 - acc: 0.9010\n",
      "Epoch 00215: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 216/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2839 - acc: 0.9010\n",
      "Epoch 00216: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 217/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2928 - acc: 0.8930\n",
      "Epoch 00217: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 218/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3349 - acc: 0.8910\n",
      "Epoch 00218: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 219/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2788 - acc: 0.9010\n",
      "Epoch 00219: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 220/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2959 - acc: 0.8820\n",
      "Epoch 00220: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 221/400\n",
      "20000/21450 [==========================>...] - ETA: 0s - loss: 0.2954 - acc: 0.8950\n",
      "Epoch 00221: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 222/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3146 - acc: 0.8990\n",
      "Epoch 00222: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 223/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3484 - acc: 0.8830\n",
      "Epoch 00223: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 224/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3076 - acc: 0.8900\n",
      "Epoch 00224: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 225/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2958 - acc: 0.8970\n",
      "Epoch 00225: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 226/400\n",
      "21000/21450 [============================>.] - ETA: 0s - loss: 0.2978 - acc: 0.8942\n",
      "Epoch 00226: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 227/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3454 - acc: 0.8840\n",
      "Epoch 00227: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 228/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3205 - acc: 0.8790\n",
      "Epoch 00228: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 229/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3059 - acc: 0.8970\n",
      "Epoch 00229: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 230/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2749 - acc: 0.9080\n",
      "Epoch 00230: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 231/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3469 - acc: 0.8830\n",
      "Epoch 00231: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 232/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3132 - acc: 0.8760\n",
      "Epoch 00232: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 233/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2736 - acc: 0.9090\n",
      "Epoch 00233: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 234/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3051 - acc: 0.8930\n",
      "Epoch 00234: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 235/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2709 - acc: 0.9070\n",
      "Epoch 00235: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 236/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2692 - acc: 0.9030\n",
      "Epoch 00236: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 237/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2849 - acc: 0.8980\n",
      "Epoch 00237: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 238/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3035 - acc: 0.8910\n",
      "Epoch 00238: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2655 - acc: 0.9030\n",
      "Epoch 00239: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 240/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3406 - acc: 0.8760\n",
      "Epoch 00240: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 241/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3206 - acc: 0.9010\n",
      "Epoch 00241: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 242/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2949 - acc: 0.8920\n",
      "Epoch 00242: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 243/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2864 - acc: 0.8950\n",
      "Epoch 00243: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 244/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2731 - acc: 0.9010\n",
      "Epoch 00244: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 245/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3491 - acc: 0.8850\n",
      "Epoch 00245: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 246/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2624 - acc: 0.9030\n",
      "Epoch 00246: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 247/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2826 - acc: 0.8920\n",
      "Epoch 00247: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 248/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2659 - acc: 0.9030\n",
      "Epoch 00248: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 249/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2780 - acc: 0.8910\n",
      "Epoch 00249: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 250/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2558 - acc: 0.9060\n",
      "Epoch 00250: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 251/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2643 - acc: 0.9170\n",
      "Epoch 00251: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 252/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2509 - acc: 0.9130\n",
      "Epoch 00252: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 253/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3033 - acc: 0.8930\n",
      "Epoch 00253: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 254/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2871 - acc: 0.8860\n",
      "Epoch 00254: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 255/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2756 - acc: 0.9040\n",
      "Epoch 00255: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 256/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3031 - acc: 0.8820\n",
      "Epoch 00256: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 257/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2866 - acc: 0.8980\n",
      "Epoch 00257: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 258/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2666 - acc: 0.9040\n",
      "Epoch 00258: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 259/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3065 - acc: 0.8820\n",
      "Epoch 00259: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 260/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3198 - acc: 0.8970\n",
      "Epoch 00260: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 261/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2867 - acc: 0.8980\n",
      "Epoch 00261: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 262/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3359 - acc: 0.8680\n",
      "Epoch 00262: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 263/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3063 - acc: 0.8830\n",
      "Epoch 00263: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 264/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2840 - acc: 0.9070\n",
      "Epoch 00264: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 265/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2971 - acc: 0.8920\n",
      "Epoch 00265: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 266/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2981 - acc: 0.8920\n",
      "Epoch 00266: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 267/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3271 - acc: 0.8870\n",
      "Epoch 00267: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 268/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3152 - acc: 0.8870\n",
      "Epoch 00268: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 269/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2675 - acc: 0.9020\n",
      "Epoch 00269: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 270/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2710 - acc: 0.9110\n",
      "Epoch 00270: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 271/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3169 - acc: 0.8840\n",
      "Epoch 00271: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 272/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2831 - acc: 0.8870\n",
      "Epoch 00272: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 273/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2873 - acc: 0.9010\n",
      "Epoch 00273: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 274/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2937 - acc: 0.9020\n",
      "Epoch 00274: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 275/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2486 - acc: 0.9150\n",
      "Epoch 00275: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 276/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3073 - acc: 0.8830\n",
      "Epoch 00276: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 277/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2645 - acc: 0.9160\n",
      "Epoch 00277: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 278/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3017 - acc: 0.8930\n",
      "Epoch 00278: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 279/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2976 - acc: 0.8950\n",
      "Epoch 00279: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 280/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3303 - acc: 0.8810\n",
      "Epoch 00280: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 281/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3262 - acc: 0.8910\n",
      "Epoch 00281: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 282/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2917 - acc: 0.9060\n",
      "Epoch 00282: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 283/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2752 - acc: 0.9000\n",
      "Epoch 00283: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 284/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2972 - acc: 0.8910\n",
      "Epoch 00284: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 285/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2989 - acc: 0.9000\n",
      "Epoch 00285: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 286/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3068 - acc: 0.9000\n",
      "Epoch 00286: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 287/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3476 - acc: 0.8760\n",
      "Epoch 00287: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 288/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3019 - acc: 0.8990\n",
      "Epoch 00288: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 289/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2819 - acc: 0.8940\n",
      "Epoch 00289: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 290/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2817 - acc: 0.8960\n",
      "Epoch 00290: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 291/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2838 - acc: 0.8940\n",
      "Epoch 00291: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 292/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2624 - acc: 0.9120\n",
      "Epoch 00292: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 293/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2809 - acc: 0.8990\n",
      "Epoch 00293: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 294/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2887 - acc: 0.8970\n",
      "Epoch 00294: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 295/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3170 - acc: 0.8860\n",
      "Epoch 00295: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 296/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2835 - acc: 0.9030\n",
      "Epoch 00296: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 297/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2709 - acc: 0.8970\n",
      "Epoch 00297: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 298/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2899 - acc: 0.8970\n",
      "Epoch 00298: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3277 - acc: 0.8950\n",
      "Epoch 00299: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 300/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2878 - acc: 0.9000\n",
      "Epoch 00300: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 301/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2906 - acc: 0.9170\n",
      "Epoch 00301: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 302/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2932 - acc: 0.8860\n",
      "Epoch 00302: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 303/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2851 - acc: 0.8950\n",
      "Epoch 00303: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 304/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2843 - acc: 0.8980\n",
      "Epoch 00304: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 305/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2605 - acc: 0.9050\n",
      "Epoch 00305: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 306/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2876 - acc: 0.9020\n",
      "Epoch 00306: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 307/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2869 - acc: 0.9050\n",
      "Epoch 00307: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 308/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2928 - acc: 0.8940\n",
      "Epoch 00308: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 309/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2725 - acc: 0.9020\n",
      "Epoch 00309: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 310/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3202 - acc: 0.8970\n",
      "Epoch 00310: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 311/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2976 - acc: 0.8910\n",
      "Epoch 00311: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 312/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2841 - acc: 0.9000\n",
      "Epoch 00312: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 313/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3232 - acc: 0.8750\n",
      "Epoch 00313: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 314/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2973 - acc: 0.8940\n",
      "Epoch 00314: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 315/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2922 - acc: 0.8830\n",
      "Epoch 00315: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 316/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3023 - acc: 0.8820\n",
      "Epoch 00316: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 317/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3064 - acc: 0.8960\n",
      "Epoch 00317: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 318/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2876 - acc: 0.8780\n",
      "Epoch 00318: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 319/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2731 - acc: 0.9110\n",
      "Epoch 00319: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 320/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2844 - acc: 0.8890\n",
      "Epoch 00320: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 321/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3085 - acc: 0.8940\n",
      "Epoch 00321: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 322/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3224 - acc: 0.8940\n",
      "Epoch 00322: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 323/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2989 - acc: 0.9000\n",
      "Epoch 00323: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 324/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2748 - acc: 0.9040\n",
      "Epoch 00324: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 325/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3104 - acc: 0.9020\n",
      "Epoch 00325: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 326/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2867 - acc: 0.9010\n",
      "Epoch 00326: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 327/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3504 - acc: 0.8850\n",
      "Epoch 00327: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 328/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3626 - acc: 0.8900\n",
      "Epoch 00328: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 329/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2751 - acc: 0.8890\n",
      "Epoch 00329: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 330/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3106 - acc: 0.8880\n",
      "Epoch 00330: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 331/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2891 - acc: 0.8890\n",
      "Epoch 00331: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 332/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3123 - acc: 0.8980\n",
      "Epoch 00332: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 333/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2805 - acc: 0.8980\n",
      "Epoch 00333: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 334/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2913 - acc: 0.8850\n",
      "Epoch 00334: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 335/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3037 - acc: 0.8830\n",
      "Epoch 00335: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 336/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2517 - acc: 0.9060\n",
      "Epoch 00336: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 337/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2679 - acc: 0.9060\n",
      "Epoch 00337: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 338/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2867 - acc: 0.9060\n",
      "Epoch 00338: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 339/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3405 - acc: 0.8950\n",
      "Epoch 00339: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 340/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2675 - acc: 0.8970\n",
      "Epoch 00340: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 341/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3207 - acc: 0.8900\n",
      "Epoch 00341: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 342/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2928 - acc: 0.9050\n",
      "Epoch 00342: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 343/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2768 - acc: 0.9100\n",
      "Epoch 00343: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 344/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2595 - acc: 0.9050\n",
      "Epoch 00344: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 345/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2971 - acc: 0.9010\n",
      "Epoch 00345: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 346/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3277 - acc: 0.9000\n",
      "Epoch 00346: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 347/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3533 - acc: 0.8950\n",
      "Epoch 00347: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 348/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3107 - acc: 0.8910\n",
      "Epoch 00348: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 349/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2993 - acc: 0.8980\n",
      "Epoch 00349: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 350/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3141 - acc: 0.8900\n",
      "Epoch 00350: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 351/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3032 - acc: 0.8850\n",
      "Epoch 00351: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 352/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3264 - acc: 0.8730\n",
      "Epoch 00352: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 353/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3308 - acc: 0.8860\n",
      "Epoch 00353: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 354/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3023 - acc: 0.8880\n",
      "Epoch 00354: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 355/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2982 - acc: 0.8910\n",
      "Epoch 00355: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 356/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3016 - acc: 0.8890\n",
      "Epoch 00356: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 357/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3521 - acc: 0.8840\n",
      "Epoch 00357: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 358/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3260 - acc: 0.8860\n",
      "Epoch 00358: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 359/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3109 - acc: 0.8970\n",
      "Epoch 00359: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 360/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2901 - acc: 0.9040\n",
      "Epoch 00360: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 361/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3075 - acc: 0.9090\n",
      "Epoch 00361: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 362/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2861 - acc: 0.8940\n",
      "Epoch 00362: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 363/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3365 - acc: 0.8820\n",
      "Epoch 00363: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 364/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2801 - acc: 0.9000\n",
      "Epoch 00364: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 365/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3078 - acc: 0.8750\n",
      "Epoch 00365: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 366/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3081 - acc: 0.8900\n",
      "Epoch 00366: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 367/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2896 - acc: 0.9010\n",
      "Epoch 00367: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 368/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2894 - acc: 0.8940\n",
      "Epoch 00368: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 369/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2874 - acc: 0.9040\n",
      "Epoch 00369: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 370/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3230 - acc: 0.8970\n",
      "Epoch 00370: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 371/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2995 - acc: 0.8940\n",
      "Epoch 00371: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 372/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3225 - acc: 0.8860\n",
      "Epoch 00372: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 373/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3302 - acc: 0.8910\n",
      "Epoch 00373: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 374/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2918 - acc: 0.9080\n",
      "Epoch 00374: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 375/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2548 - acc: 0.9140\n",
      "Epoch 00375: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 376/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3084 - acc: 0.8960\n",
      "Epoch 00376: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 377/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3040 - acc: 0.8900\n",
      "Epoch 00377: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 378/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2823 - acc: 0.8930\n",
      "Epoch 00378: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 379/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3092 - acc: 0.8960\n",
      "Epoch 00379: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 380/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3105 - acc: 0.8920\n",
      "Epoch 00380: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 381/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3078 - acc: 0.8960\n",
      "Epoch 00381: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 382/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2676 - acc: 0.8980\n",
      "Epoch 00382: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 383/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3164 - acc: 0.8980\n",
      "Epoch 00383: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 384/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3351 - acc: 0.8860\n",
      "Epoch 00384: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 385/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3015 - acc: 0.8860\n",
      "Epoch 00385: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 386/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2756 - acc: 0.8910\n",
      "Epoch 00386: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 387/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3023 - acc: 0.8840\n",
      "Epoch 00387: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 388/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3012 - acc: 0.8920\n",
      "Epoch 00388: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 389/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3198 - acc: 0.8890\n",
      "Epoch 00389: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 390/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2547 - acc: 0.9050\n",
      "Epoch 00390: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 391/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2629 - acc: 0.9030\n",
      "Epoch 00391: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 392/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3082 - acc: 0.8890\n",
      "Epoch 00392: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 393/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2738 - acc: 0.9030\n",
      "Epoch 00393: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 394/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3123 - acc: 0.8930\n",
      "Epoch 00394: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 395/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2886 - acc: 0.8840\n",
      "Epoch 00395: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 396/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3054 - acc: 0.8920\n",
      "Epoch 00396: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 397/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2649 - acc: 0.9040\n",
      "Epoch 00397: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 398/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2829 - acc: 0.9100\n",
      "Epoch 00398: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 399/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2742 - acc: 0.9050\n",
      "Epoch 00399: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n",
      "Epoch 400/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2999 - acc: 0.8930\n",
      "Epoch 00400: val_loss did not improve from 0.29075\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2974 - acc: 0.8945 - val_loss: 0.3086 - val_acc: 0.8961\n"
     ]
    }
   ],
   "source": [
    "bz = 1000\n",
    "epochs = 400\n",
    "history = model.fit(\n",
    "    xtrain,\n",
    "    ytrain,\n",
    "    epochs = epochs,\n",
    "    batch_size = bz,\n",
    "    validation_data =(xtest, ytest),\n",
    "    callbacks = [mcp, red_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7150/7150 [==============================] - 0s 20us/sample - loss: 0.2510 - acc: 0.8972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2509558886331278, 0.8972028]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss Vs # of epochs')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "plt.plot( [i for i in range(epochs)], history.history['val_loss'], label = \"validation loss\")\n",
    "plt.xlabel(\"#of epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Loss Vs # of epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x23b706db390>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(cos_d, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss Vs # of epochs')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(epochs)], history.history['loss'], label = \"Train loss\")\n",
    "plt.xlabel(\"#of epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Loss Vs # of epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Learning rate Vs # of epochs')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(epochs)], history.history['lr'], label = \"Train loss\")\n",
    "plt.xlabel(\"#of epochs\")\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.title(\"Learning rate Vs # of epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 15us/sample - loss: 0.2911 - acc: 0.8828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2910575838883718, 0.88283336]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc', 'lr'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best.load_weights(\"models_chpt/best_modelD.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 16us/sample - loss: 0.2911 - acc: 0.8828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2910575842857361, 0.88283336]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_best.evaluate(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision1",
   "language": "python",
   "name": "vision1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
