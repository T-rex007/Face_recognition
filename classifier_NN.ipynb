{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from VisionUtils import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat1 = np.load(\"features/feat1.npy\")\n",
    "feat2 = np.load(\"features/feat2.npy\")\n",
    "labels = np.load(\"features/labels.npy\")\n",
    "feat3 = np.load(\"features/feat3.npy\")\n",
    "feat4 = np.load(\"features/feat4.npy\")\n",
    "labels34 = np.load(\"features/labels34.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feat1.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = [feat1[i1].reshape(-1,1) for i1 in range(len(feat1))]\n",
    "f2 = [feat2[i2].reshape(-1,1) for i2 in range(len(feat2))]\n",
    "cos_d = np.array(\n",
    "    [ feat_distance_cosine_scalar(f1[i].T, f2[i]) for i in range(len(feat1))]).reshape(-1,1)\n",
    "eucl_d = np.array([ np.linalg.norm(f1[i]- f2[i]) for i in range(len(feat1))]).reshape(-1,1)\n",
    "sqr_diff = np.power(np.abs(feat1-feat2), 2)\n",
    "\n",
    "data_dict = { \"labels\": labels,\n",
    "             \"cosine_distance\": cos_d, \n",
    "             \"Eucledian_distance\": eucl_d,\n",
    "             \"sqaured_difference\":sqr_diff}\n",
    "data = np.hstack([cos_d,eucl_d, sqr_diff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "feats = SelectKBest(chi2, k = int(0.8*(len(data.T)))).fit_transform(scaled_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eucl_d[:100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x294a62f7b38>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(cos_d, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(feats, labels, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "he_init = tf.keras.initializers.VarianceScaling()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation = 'relu', kernel_initializer = he_init, \n",
    "                kernel_regularizer = l1(0.001), \n",
    "                #input_shape = (1,128)\n",
    "               ))\n",
    "model.add(Dense(5, activation = 'relu',kernel_initializer = he_init))\n",
    "model.add(Dense(1, activation = \"sigmoid\", kernel_initializer = he_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "mcp = ModelCheckpoint(\"models_chpt/best_modelD.hdf5\",verbose = 1, \n",
    "                      monitor = \"val_loss\", save_best_only = True, save_weights_only = True)\n",
    "red_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor =0.5)\n",
    "opt = tf.keras.optimizers.Adam(0.01)\n",
    "model.compile(optimizer = opt, loss = \"binary_crossentropy\", \n",
    "              metrics = [\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37500 samples, validate on 12500 samples\n",
      "Epoch 1/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.6674 - acc: 0.7489\n",
      "Epoch 00001: val_loss improved from inf to 0.45829, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 8us/sample - loss: 0.6337 - acc: 0.7631 - val_loss: 0.4583 - val_acc: 0.8310\n",
      "Epoch 2/400\n",
      "29000/37500 [======================>.......] - ETA: 0s - loss: 0.4300 - acc: 0.8302\n",
      "Epoch 00002: val_loss improved from 0.45829 to 0.40375, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.4229 - acc: 0.8330 - val_loss: 0.4038 - val_acc: 0.8321\n",
      "Epoch 3/400\n",
      "28000/37500 [=====================>........] - ETA: 0s - loss: 0.4004 - acc: 0.8346\n",
      "Epoch 00003: val_loss improved from 0.40375 to 0.40225, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.4011 - acc: 0.8334 - val_loss: 0.4023 - val_acc: 0.8297\n",
      "Epoch 4/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3938 - acc: 0.8351\n",
      "Epoch 00004: val_loss improved from 0.40225 to 0.39718, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3951 - acc: 0.8347 - val_loss: 0.3972 - val_acc: 0.8311\n",
      "Epoch 5/400\n",
      "28000/37500 [=====================>........] - ETA: 0s - loss: 0.3948 - acc: 0.8349\n",
      "Epoch 00005: val_loss did not improve from 0.39718\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3953 - acc: 0.8342 - val_loss: 0.3984 - val_acc: 0.8311\n",
      "Epoch 6/400\n",
      "29000/37500 [======================>.......] - ETA: 0s - loss: 0.3948 - acc: 0.8347\n",
      "Epoch 00006: val_loss improved from 0.39718 to 0.39634, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3952 - acc: 0.8339 - val_loss: 0.3963 - val_acc: 0.8312\n",
      "Epoch 7/400\n",
      "25000/37500 [===================>..........] - ETA: 0s - loss: 0.3962 - acc: 0.8334\n",
      "Epoch 00007: val_loss did not improve from 0.39634\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3943 - acc: 0.8341 - val_loss: 0.3967 - val_acc: 0.8316\n",
      "Epoch 8/400\n",
      "29000/37500 [======================>.......] - ETA: 0s - loss: 0.3951 - acc: 0.8330\n",
      "Epoch 00008: val_loss improved from 0.39634 to 0.39546, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3959 - acc: 0.8330 - val_loss: 0.3955 - val_acc: 0.8309\n",
      "Epoch 9/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3944 - acc: 0.8334\n",
      "Epoch 00009: val_loss did not improve from 0.39546\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3946 - acc: 0.8335 - val_loss: 0.4067 - val_acc: 0.8278\n",
      "Epoch 10/400\n",
      "29000/37500 [======================>.......] - ETA: 0s - loss: 0.3926 - acc: 0.8339\n",
      "Epoch 00010: val_loss did not improve from 0.39546\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3940 - acc: 0.8331 - val_loss: 0.3984 - val_acc: 0.8300\n",
      "Epoch 11/400\n",
      "29000/37500 [======================>.......] - ETA: 0s - loss: 0.3930 - acc: 0.8336\n",
      "Epoch 00011: val_loss improved from 0.39546 to 0.39356, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3934 - acc: 0.8340 - val_loss: 0.3936 - val_acc: 0.8313\n",
      "Epoch 12/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3911 - acc: 0.8348\n",
      "Epoch 00012: val_loss did not improve from 0.39356\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3917 - acc: 0.8341 - val_loss: 0.3953 - val_acc: 0.8306\n",
      "Epoch 13/400\n",
      "29000/37500 [======================>.......] - ETA: 0s - loss: 0.3905 - acc: 0.8343\n",
      "Epoch 00013: val_loss improved from 0.39356 to 0.39268, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3910 - acc: 0.8333 - val_loss: 0.3927 - val_acc: 0.8310\n",
      "Epoch 14/400\n",
      "27000/37500 [====================>.........] - ETA: 0s - loss: 0.3902 - acc: 0.8346\n",
      "Epoch 00014: val_loss improved from 0.39268 to 0.39177, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3905 - acc: 0.8335 - val_loss: 0.3918 - val_acc: 0.8310\n",
      "Epoch 15/400\n",
      "28000/37500 [=====================>........] - ETA: 0s - loss: 0.3916 - acc: 0.8326\n",
      "Epoch 00015: val_loss improved from 0.39177 to 0.39034, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3901 - acc: 0.8334 - val_loss: 0.3903 - val_acc: 0.8316\n",
      "Epoch 16/400\n",
      "29000/37500 [======================>.......] - ETA: 0s - loss: 0.3884 - acc: 0.8333\n",
      "Epoch 00016: val_loss improved from 0.39034 to 0.38876, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3889 - acc: 0.8329 - val_loss: 0.3888 - val_acc: 0.8310\n",
      "Epoch 17/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3911 - acc: 0.8314\n",
      "Epoch 00017: val_loss did not improve from 0.38876\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3900 - acc: 0.8319 - val_loss: 0.4053 - val_acc: 0.8266\n",
      "Epoch 18/400\n",
      "24000/37500 [==================>...........] - ETA: 0s - loss: 0.3869 - acc: 0.8336\n",
      "Epoch 00018: val_loss did not improve from 0.38876\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3892 - acc: 0.8329 - val_loss: 0.4015 - val_acc: 0.8270\n",
      "Epoch 19/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3909 - acc: 0.8311\n",
      "Epoch 00019: val_loss improved from 0.38876 to 0.38722, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3894 - acc: 0.8318 - val_loss: 0.3872 - val_acc: 0.8302\n",
      "Epoch 20/400\n",
      "29000/37500 [======================>.......] - ETA: 0s - loss: 0.3840 - acc: 0.8340\n",
      "Epoch 00020: val_loss improved from 0.38722 to 0.38329, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3832 - acc: 0.8343 - val_loss: 0.3833 - val_acc: 0.8302\n",
      "Epoch 21/400\n",
      "28000/37500 [=====================>........] - ETA: 0s - loss: 0.3837 - acc: 0.8336\n",
      "Epoch 00021: val_loss improved from 0.38329 to 0.38299, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3833 - acc: 0.8336 - val_loss: 0.3830 - val_acc: 0.8304\n",
      "Epoch 22/400\n",
      "28000/37500 [=====================>........] - ETA: 0s - loss: 0.3814 - acc: 0.8342\n",
      "Epoch 00022: val_loss did not improve from 0.38299\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3824 - acc: 0.8336 - val_loss: 0.3879 - val_acc: 0.8294\n",
      "Epoch 23/400\n",
      "26000/37500 [===================>..........] - ETA: 0s - loss: 0.3831 - acc: 0.8311\n",
      "Epoch 00023: val_loss did not improve from 0.38299\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3811 - acc: 0.8338 - val_loss: 0.3856 - val_acc: 0.8301\n",
      "Epoch 24/400\n",
      "26000/37500 [===================>..........] - ETA: 0s - loss: 0.3746 - acc: 0.8373\n",
      "Epoch 00024: val_loss improved from 0.38299 to 0.38096, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3783 - acc: 0.8350 - val_loss: 0.3810 - val_acc: 0.8311\n",
      "Epoch 25/400\n",
      "27000/37500 [====================>.........] - ETA: 0s - loss: 0.3769 - acc: 0.8350\n",
      "Epoch 00025: val_loss improved from 0.38096 to 0.37980, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3763 - acc: 0.8351 - val_loss: 0.3798 - val_acc: 0.8310\n",
      "Epoch 26/400\n",
      "28000/37500 [=====================>........] - ETA: 0s - loss: 0.3785 - acc: 0.8345\n",
      "Epoch 00026: val_loss improved from 0.37980 to 0.37683, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3779 - acc: 0.8343 - val_loss: 0.3768 - val_acc: 0.8314\n",
      "Epoch 27/400\n",
      "27000/37500 [====================>.........] - ETA: 0s - loss: 0.3739 - acc: 0.8341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00027: val_loss improved from 0.37683 to 0.37668, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3753 - acc: 0.8339 - val_loss: 0.3767 - val_acc: 0.8318\n",
      "Epoch 28/400\n",
      "28000/37500 [=====================>........] - ETA: 0s - loss: 0.3719 - acc: 0.8365\n",
      "Epoch 00028: val_loss improved from 0.37668 to 0.37596, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3747 - acc: 0.8339 - val_loss: 0.3760 - val_acc: 0.8311\n",
      "Epoch 29/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3728 - acc: 0.8332\n",
      "Epoch 00029: val_loss improved from 0.37596 to 0.37385, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3742 - acc: 0.8324 - val_loss: 0.3738 - val_acc: 0.8306\n",
      "Epoch 30/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3734 - acc: 0.8340\n",
      "Epoch 00030: val_loss improved from 0.37385 to 0.37199, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3723 - acc: 0.8346 - val_loss: 0.3720 - val_acc: 0.8316\n",
      "Epoch 31/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3711 - acc: 0.8335\n",
      "Epoch 00031: val_loss did not improve from 0.37199\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3701 - acc: 0.8340 - val_loss: 0.3917 - val_acc: 0.8249\n",
      "Epoch 32/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3766 - acc: 0.8313\n",
      "Epoch 00032: val_loss improved from 0.37199 to 0.37137, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 4us/sample - loss: 0.3736 - acc: 0.8325 - val_loss: 0.3714 - val_acc: 0.8318\n",
      "Epoch 33/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3715 - acc: 0.8340\n",
      "Epoch 00033: val_loss did not improve from 0.37137\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3705 - acc: 0.8339 - val_loss: 0.3724 - val_acc: 0.8316\n",
      "Epoch 34/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3689 - acc: 0.8352\n",
      "Epoch 00034: val_loss did not improve from 0.37137\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3700 - acc: 0.8346 - val_loss: 0.3775 - val_acc: 0.8290\n",
      "Epoch 35/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3733 - acc: 0.8318\n",
      "Epoch 00035: val_loss improved from 0.37137 to 0.36937, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3715 - acc: 0.8327 - val_loss: 0.3694 - val_acc: 0.8314\n",
      "Epoch 36/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3658 - acc: 0.8360\n",
      "Epoch 00036: val_loss improved from 0.36937 to 0.36742, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3667 - acc: 0.8357 - val_loss: 0.3674 - val_acc: 0.8317\n",
      "Epoch 37/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3692 - acc: 0.8324\n",
      "Epoch 00037: val_loss did not improve from 0.36742\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3675 - acc: 0.8338 - val_loss: 0.3695 - val_acc: 0.8302\n",
      "Epoch 38/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3662 - acc: 0.8335\n",
      "Epoch 00038: val_loss did not improve from 0.36742\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3687 - acc: 0.8330 - val_loss: 0.3799 - val_acc: 0.8270\n",
      "Epoch 39/400\n",
      "30000/37500 [=======================>......] - ETA: 0s - loss: 0.3708 - acc: 0.8326\n",
      "Epoch 00039: val_loss improved from 0.36742 to 0.36688, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3696 - acc: 0.8339 - val_loss: 0.3669 - val_acc: 0.8314\n",
      "Epoch 40/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3638 - acc: 0.8366\n",
      "Epoch 00040: val_loss improved from 0.36688 to 0.36618, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3653 - acc: 0.8358 - val_loss: 0.3662 - val_acc: 0.8314\n",
      "Epoch 41/400\n",
      "29000/37500 [======================>.......] - ETA: 0s - loss: 0.3587 - acc: 0.831 - ETA: 0s - loss: 0.3653 - acc: 0.8357\n",
      "Epoch 00041: val_loss did not improve from 0.36618\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3649 - acc: 0.8358 - val_loss: 0.3666 - val_acc: 0.8331\n",
      "Epoch 42/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3619 - acc: 0.835 - ETA: 0s - loss: 0.3671 - acc: 0.8325\n",
      "Epoch 00042: val_loss did not improve from 0.36618\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3662 - acc: 0.8327 - val_loss: 0.3704 - val_acc: 0.8286\n",
      "Epoch 43/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3666 - acc: 0.8323\n",
      "Epoch 00043: val_loss did not improve from 0.36618\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3664 - acc: 0.8330 - val_loss: 0.3735 - val_acc: 0.8286\n",
      "Epoch 44/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3659 - acc: 0.8340\n",
      "Epoch 00044: val_loss did not improve from 0.36618\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3650 - acc: 0.8338 - val_loss: 0.3669 - val_acc: 0.8303\n",
      "Epoch 45/400\n",
      "28000/37500 [=====================>........] - ETA: 0s - loss: 0.3665 - acc: 0.8355\n",
      "Epoch 00045: val_loss did not improve from 0.36618\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3651 - acc: 0.8349 - val_loss: 0.3667 - val_acc: 0.8312\n",
      "Epoch 46/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3634 - acc: 0.8332\n",
      "Epoch 00046: val_loss improved from 0.36618 to 0.36551, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3645 - acc: 0.8331 - val_loss: 0.3655 - val_acc: 0.8303\n",
      "Epoch 47/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3632 - acc: 0.8338\n",
      "Epoch 00047: val_loss improved from 0.36551 to 0.36473, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3637 - acc: 0.8341 - val_loss: 0.3647 - val_acc: 0.8312\n",
      "Epoch 48/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3643 - acc: 0.8335\n",
      "Epoch 00048: val_loss did not improve from 0.36473\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3660 - acc: 0.8331 - val_loss: 0.3732 - val_acc: 0.8264\n",
      "Epoch 49/400\n",
      "30000/37500 [=======================>......] - ETA: 0s - loss: 0.3658 - acc: 0.8321\n",
      "Epoch 00049: val_loss did not improve from 0.36473\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3636 - acc: 0.8337 - val_loss: 0.3657 - val_acc: 0.8321\n",
      "Epoch 50/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3629 - acc: 0.8337\n",
      "Epoch 00050: val_loss did not improve from 0.36473\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3657 - acc: 0.8331 - val_loss: 0.3657 - val_acc: 0.8310\n",
      "Epoch 51/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3642 - acc: 0.8348\n",
      "Epoch 00051: val_loss did not improve from 0.36473\n",
      "37500/37500 [==============================] - 0s 6us/sample - loss: 0.3652 - acc: 0.8348 - val_loss: 0.3674 - val_acc: 0.8300\n",
      "Epoch 52/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3647 - acc: 0.8332\n",
      "Epoch 00052: val_loss did not improve from 0.36473\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3641 - acc: 0.8337 - val_loss: 0.3682 - val_acc: 0.8287\n",
      "Epoch 53/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3667 - acc: 0.8335\n",
      "Epoch 00053: val_loss did not improve from 0.36473\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3651 - acc: 0.8337 - val_loss: 0.3659 - val_acc: 0.8314\n",
      "Epoch 54/400\n",
      "30000/37500 [=======================>......] - ETA: 0s - loss: 0.3638 - acc: 0.8339\n",
      "Epoch 00054: val_loss did not improve from 0.36473\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3637 - acc: 0.8336 - val_loss: 0.3705 - val_acc: 0.8279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/400\n",
      "30000/37500 [=======================>......] - ETA: 0s - loss: 0.3613 - acc: 0.8364\n",
      "Epoch 00055: val_loss improved from 0.36473 to 0.36380, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3621 - acc: 0.8361 - val_loss: 0.3638 - val_acc: 0.8328\n",
      "Epoch 56/400\n",
      "30000/37500 [=======================>......] - ETA: 0s - loss: 0.3667 - acc: 0.8333\n",
      "Epoch 00056: val_loss did not improve from 0.36380\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3668 - acc: 0.8331 - val_loss: 0.3767 - val_acc: 0.8266\n",
      "Epoch 57/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3652 - acc: 0.8313\n",
      "Epoch 00057: val_loss did not improve from 0.36380\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3662 - acc: 0.8317 - val_loss: 0.3730 - val_acc: 0.8272\n",
      "Epoch 58/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3669 - acc: 0.8329\n",
      "Epoch 00058: val_loss did not improve from 0.36380\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3664 - acc: 0.8335 - val_loss: 0.3663 - val_acc: 0.8311\n",
      "Epoch 59/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3616 - acc: 0.8364\n",
      "Epoch 00059: val_loss improved from 0.36380 to 0.36318, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3611 - acc: 0.8358 - val_loss: 0.3632 - val_acc: 0.8318\n",
      "Epoch 60/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3618 - acc: 0.8352\n",
      "Epoch 00060: val_loss did not improve from 0.36318\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3626 - acc: 0.8346 - val_loss: 0.3692 - val_acc: 0.8292\n",
      "Epoch 61/400\n",
      "30000/37500 [=======================>......] - ETA: 0s - loss: 0.3636 - acc: 0.8329\n",
      "Epoch 00061: val_loss did not improve from 0.36318\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3649 - acc: 0.8320 - val_loss: 0.3693 - val_acc: 0.8288\n",
      "Epoch 62/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3627 - acc: 0.8337\n",
      "Epoch 00062: val_loss did not improve from 0.36318\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3638 - acc: 0.8335 - val_loss: 0.3672 - val_acc: 0.8298\n",
      "Epoch 63/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3637 - acc: 0.8336\n",
      "Epoch 00063: val_loss did not improve from 0.36318\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3631 - acc: 0.8346 - val_loss: 0.3670 - val_acc: 0.8279\n",
      "Epoch 64/400\n",
      "29000/37500 [======================>.......] - ETA: 0s - loss: 0.3632 - acc: 0.8341\n",
      "Epoch 00064: val_loss did not improve from 0.36318\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3645 - acc: 0.8333 - val_loss: 0.3762 - val_acc: 0.8294\n",
      "Epoch 65/400\n",
      "29000/37500 [======================>.......] - ETA: 0s - loss: 0.3662 - acc: 0.8303\n",
      "Epoch 00065: val_loss did not improve from 0.36318\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3686 - acc: 0.8302 - val_loss: 0.3702 - val_acc: 0.8289\n",
      "Epoch 66/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3655 - acc: 0.8333\n",
      "Epoch 00066: val_loss did not improve from 0.36318\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3676 - acc: 0.8325 - val_loss: 0.3962 - val_acc: 0.8142\n",
      "Epoch 67/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3697 - acc: 0.8299\n",
      "Epoch 00067: val_loss did not improve from 0.36318\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3663 - acc: 0.8321 - val_loss: 0.3649 - val_acc: 0.8311\n",
      "Epoch 68/400\n",
      "29000/37500 [======================>.......] - ETA: 0s - loss: 0.3618 - acc: 0.8338\n",
      "Epoch 00068: val_loss did not improve from 0.36318\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3648 - acc: 0.8325 - val_loss: 0.3639 - val_acc: 0.8311\n",
      "Epoch 69/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3639 - acc: 0.8322\n",
      "Epoch 00069: val_loss did not improve from 0.36318\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3637 - acc: 0.8331 - val_loss: 0.3673 - val_acc: 0.8307\n",
      "Epoch 70/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3604 - acc: 0.8341\n",
      "Epoch 00070: val_loss did not improve from 0.36318\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3603 - acc: 0.8347 - val_loss: 0.3642 - val_acc: 0.8287\n",
      "Epoch 71/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3590 - acc: 0.8335\n",
      "Epoch 00071: val_loss did not improve from 0.36318\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3579 - acc: 0.8348 - val_loss: 0.3639 - val_acc: 0.8302\n",
      "Epoch 72/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3582 - acc: 0.8343\n",
      "Epoch 00072: val_loss improved from 0.36318 to 0.35991, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3574 - acc: 0.8351 - val_loss: 0.3599 - val_acc: 0.8312\n",
      "Epoch 73/400\n",
      "29000/37500 [======================>.......] - ETA: 0s - loss: 0.3595 - acc: 0.8343\n",
      "Epoch 00073: val_loss did not improve from 0.35991\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3585 - acc: 0.8346 - val_loss: 0.3627 - val_acc: 0.8305\n",
      "Epoch 74/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3554 - acc: 0.8364\n",
      "Epoch 00074: val_loss did not improve from 0.35991\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3575 - acc: 0.8354 - val_loss: 0.3599 - val_acc: 0.8314\n",
      "Epoch 75/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3578 - acc: 0.8350\n",
      "Epoch 00075: val_loss did not improve from 0.35991\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3580 - acc: 0.8353 - val_loss: 0.3611 - val_acc: 0.8315\n",
      "Epoch 76/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3563 - acc: 0.8383\n",
      "Epoch 00076: val_loss improved from 0.35991 to 0.35973, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3575 - acc: 0.8367 - val_loss: 0.3597 - val_acc: 0.8322\n",
      "Epoch 77/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3581 - acc: 0.8354\n",
      "Epoch 00077: val_loss did not improve from 0.35973\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3580 - acc: 0.8353 - val_loss: 0.3636 - val_acc: 0.8301\n",
      "Epoch 78/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3585 - acc: 0.8348\n",
      "Epoch 00078: val_loss improved from 0.35973 to 0.35957, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 6us/sample - loss: 0.3572 - acc: 0.8352 - val_loss: 0.3596 - val_acc: 0.8314\n",
      "Epoch 79/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3565 - acc: 0.8355\n",
      "Epoch 00079: val_loss did not improve from 0.35957\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3567 - acc: 0.8355 - val_loss: 0.3651 - val_acc: 0.8284\n",
      "Epoch 80/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3563 - acc: 0.8353\n",
      "Epoch 00080: val_loss did not improve from 0.35957\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3572 - acc: 0.8347 - val_loss: 0.3598 - val_acc: 0.8323\n",
      "Epoch 81/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3592 - acc: 0.8339\n",
      "Epoch 00081: val_loss improved from 0.35957 to 0.35954, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3590 - acc: 0.8336 - val_loss: 0.3595 - val_acc: 0.8318\n",
      "Epoch 82/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3573 - acc: 0.8359\n",
      "Epoch 00082: val_loss did not improve from 0.35954\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3572 - acc: 0.8361 - val_loss: 0.3612 - val_acc: 0.8322\n",
      "Epoch 83/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3552 - acc: 0.8365\n",
      "Epoch 00083: val_loss did not improve from 0.35954\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3565 - acc: 0.8354 - val_loss: 0.3624 - val_acc: 0.8310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3576 - acc: 0.8357\n",
      "Epoch 00084: val_loss improved from 0.35954 to 0.35893, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3566 - acc: 0.8363 - val_loss: 0.3589 - val_acc: 0.8318\n",
      "Epoch 85/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3576 - acc: 0.8331\n",
      "Epoch 00085: val_loss did not improve from 0.35893\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3582 - acc: 0.8335 - val_loss: 0.3621 - val_acc: 0.8306\n",
      "Epoch 86/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3610 - acc: 0.8338\n",
      "Epoch 00086: val_loss did not improve from 0.35893\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3593 - acc: 0.8343 - val_loss: 0.3654 - val_acc: 0.8300\n",
      "Epoch 87/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3577 - acc: 0.8344\n",
      "Epoch 00087: val_loss did not improve from 0.35893\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3583 - acc: 0.8340 - val_loss: 0.3596 - val_acc: 0.8326\n",
      "Epoch 88/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3569 - acc: 0.8361\n",
      "Epoch 00088: val_loss did not improve from 0.35893\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3571 - acc: 0.8357 - val_loss: 0.3610 - val_acc: 0.8318\n",
      "Epoch 89/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3568 - acc: 0.8367\n",
      "Epoch 00089: val_loss did not improve from 0.35893\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3571 - acc: 0.8366 - val_loss: 0.3595 - val_acc: 0.8309\n",
      "Epoch 90/400\n",
      " 1000/37500 [..............................] - ETA: 0s - loss: 0.3554 - acc: 0.8450\n",
      "Epoch 00090: val_loss did not improve from 0.35893\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3570 - acc: 0.8370 - val_loss: 0.3614 - val_acc: 0.8318\n",
      "Epoch 91/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3575 - acc: 0.8350\n",
      "Epoch 00091: val_loss did not improve from 0.35893\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3575 - acc: 0.8348 - val_loss: 0.3606 - val_acc: 0.8317\n",
      "Epoch 92/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3569 - acc: 0.8357\n",
      "Epoch 00092: val_loss did not improve from 0.35893\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3573 - acc: 0.8357 - val_loss: 0.3605 - val_acc: 0.8319\n",
      "Epoch 93/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3574 - acc: 0.8347\n",
      "Epoch 00093: val_loss did not improve from 0.35893\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3575 - acc: 0.8344 - val_loss: 0.3611 - val_acc: 0.8318\n",
      "Epoch 94/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3564 - acc: 0.8376\n",
      "Epoch 00094: val_loss did not improve from 0.35893\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3573 - acc: 0.8362 - val_loss: 0.3615 - val_acc: 0.8302\n",
      "Epoch 95/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3542 - acc: 0.8370\n",
      "Epoch 00095: val_loss improved from 0.35893 to 0.35748, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3545 - acc: 0.8367 - val_loss: 0.3575 - val_acc: 0.8322\n",
      "Epoch 96/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3540 - acc: 0.8364\n",
      "Epoch 00096: val_loss improved from 0.35748 to 0.35745, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3537 - acc: 0.8365 - val_loss: 0.3575 - val_acc: 0.8323\n",
      "Epoch 97/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3551 - acc: 0.8344\n",
      "Epoch 00097: val_loss improved from 0.35745 to 0.35738, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3550 - acc: 0.8348 - val_loss: 0.3574 - val_acc: 0.8316\n",
      "Epoch 98/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3530 - acc: 0.8362\n",
      "Epoch 00098: val_loss did not improve from 0.35738\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3534 - acc: 0.8359 - val_loss: 0.3591 - val_acc: 0.8301\n",
      "Epoch 99/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3541 - acc: 0.8373\n",
      "Epoch 00099: val_loss did not improve from 0.35738\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3542 - acc: 0.8373 - val_loss: 0.3590 - val_acc: 0.8314\n",
      "Epoch 100/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3526 - acc: 0.8372\n",
      "Epoch 00100: val_loss did not improve from 0.35738\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3538 - acc: 0.8367 - val_loss: 0.3576 - val_acc: 0.8319\n",
      "Epoch 101/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3529 - acc: 0.8361\n",
      "Epoch 00101: val_loss improved from 0.35738 to 0.35715, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3538 - acc: 0.8354 - val_loss: 0.3572 - val_acc: 0.8321\n",
      "Epoch 102/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3544 - acc: 0.8356\n",
      "Epoch 00102: val_loss did not improve from 0.35715\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3543 - acc: 0.8356 - val_loss: 0.3590 - val_acc: 0.8305\n",
      "Epoch 103/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3540 - acc: 0.8366\n",
      "Epoch 00103: val_loss did not improve from 0.35715\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3541 - acc: 0.8366 - val_loss: 0.3604 - val_acc: 0.8295\n",
      "Epoch 104/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3546 - acc: 0.8356\n",
      "Epoch 00104: val_loss did not improve from 0.35715\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3545 - acc: 0.8358 - val_loss: 0.3583 - val_acc: 0.8321\n",
      "Epoch 105/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3545 - acc: 0.8369\n",
      "Epoch 00105: val_loss did not improve from 0.35715\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3545 - acc: 0.8372 - val_loss: 0.3573 - val_acc: 0.8315\n",
      "Epoch 106/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3550 - acc: 0.8364\n",
      "Epoch 00106: val_loss did not improve from 0.35715\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3544 - acc: 0.8365 - val_loss: 0.3577 - val_acc: 0.8321\n",
      "Epoch 107/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3544 - acc: 0.8374\n",
      "Epoch 00107: val_loss did not improve from 0.35715\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3543 - acc: 0.8372 - val_loss: 0.3572 - val_acc: 0.8322\n",
      "Epoch 108/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3531 - acc: 0.8377\n",
      "Epoch 00108: val_loss did not improve from 0.35715\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3533 - acc: 0.8375 - val_loss: 0.3576 - val_acc: 0.8329\n",
      "Epoch 109/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3551 - acc: 0.8364\n",
      "Epoch 00109: val_loss did not improve from 0.35715\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3541 - acc: 0.8370 - val_loss: 0.3581 - val_acc: 0.8317\n",
      "Epoch 110/400\n",
      "29000/37500 [======================>.......] - ETA: 0s - loss: 0.3558 - acc: 0.8372\n",
      "Epoch 00110: val_loss did not improve from 0.35715\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3537 - acc: 0.8372 - val_loss: 0.3607 - val_acc: 0.8307\n",
      "Epoch 111/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3535 - acc: 0.8369\n",
      "Epoch 00111: val_loss did not improve from 0.35715\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3536 - acc: 0.8367 - val_loss: 0.3575 - val_acc: 0.8319\n",
      "Epoch 112/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3524 - acc: 0.8368\n",
      "Epoch 00112: val_loss did not improve from 0.35715\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3520 - acc: 0.8370 - val_loss: 0.3574 - val_acc: 0.8310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3529 - acc: 0.8363\n",
      "Epoch 00113: val_loss improved from 0.35715 to 0.35632, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3522 - acc: 0.8371 - val_loss: 0.3563 - val_acc: 0.8318\n",
      "Epoch 114/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3513 - acc: 0.8381\n",
      "Epoch 00114: val_loss did not improve from 0.35632\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3517 - acc: 0.8373 - val_loss: 0.3570 - val_acc: 0.8320\n",
      "Epoch 115/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3517 - acc: 0.8367\n",
      "Epoch 00115: val_loss did not improve from 0.35632\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3524 - acc: 0.8362 - val_loss: 0.3579 - val_acc: 0.8295\n",
      "Epoch 116/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3531 - acc: 0.8371\n",
      "Epoch 00116: val_loss improved from 0.35632 to 0.35615, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3524 - acc: 0.8375 - val_loss: 0.3561 - val_acc: 0.8321\n",
      "Epoch 117/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3513 - acc: 0.8378\n",
      "Epoch 00117: val_loss improved from 0.35615 to 0.35610, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3516 - acc: 0.8376 - val_loss: 0.3561 - val_acc: 0.8311\n",
      "Epoch 118/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3543 - acc: 0.8360\n",
      "Epoch 00118: val_loss improved from 0.35610 to 0.35592, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3517 - acc: 0.8374 - val_loss: 0.3559 - val_acc: 0.8322\n",
      "Epoch 119/400\n",
      "30000/37500 [=======================>......] - ETA: 0s - loss: 0.3514 - acc: 0.8377\n",
      "Epoch 00119: val_loss did not improve from 0.35592\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3517 - acc: 0.8372 - val_loss: 0.3570 - val_acc: 0.8321\n",
      "Epoch 120/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3506 - acc: 0.8369\n",
      "Epoch 00120: val_loss did not improve from 0.35592\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3523 - acc: 0.8367 - val_loss: 0.3564 - val_acc: 0.8318\n",
      "Epoch 121/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3525 - acc: 0.8363\n",
      "Epoch 00121: val_loss did not improve from 0.35592\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3518 - acc: 0.8371 - val_loss: 0.3560 - val_acc: 0.8313\n",
      "Epoch 122/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3518 - acc: 0.8370\n",
      "Epoch 00122: val_loss did not improve from 0.35592\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3513 - acc: 0.8373 - val_loss: 0.3561 - val_acc: 0.8322\n",
      "Epoch 123/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3508 - acc: 0.8378\n",
      "Epoch 00123: val_loss did not improve from 0.35592\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3517 - acc: 0.8374 - val_loss: 0.3576 - val_acc: 0.8318\n",
      "Epoch 124/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3526 - acc: 0.8368\n",
      "Epoch 00124: val_loss did not improve from 0.35592\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3526 - acc: 0.8374 - val_loss: 0.3560 - val_acc: 0.8321\n",
      "Epoch 125/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3527 - acc: 0.8356\n",
      "Epoch 00125: val_loss did not improve from 0.35592\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3528 - acc: 0.8357 - val_loss: 0.3565 - val_acc: 0.8319\n",
      "Epoch 126/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3510 - acc: 0.8378\n",
      "Epoch 00126: val_loss did not improve from 0.35592\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3518 - acc: 0.8372 - val_loss: 0.3566 - val_acc: 0.8323\n",
      "Epoch 127/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3510 - acc: 0.8378\n",
      "Epoch 00127: val_loss did not improve from 0.35592\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3515 - acc: 0.8375 - val_loss: 0.3561 - val_acc: 0.8320\n",
      "Epoch 128/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3517 - acc: 0.8373\n",
      "Epoch 00128: val_loss did not improve from 0.35592\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3516 - acc: 0.8373 - val_loss: 0.3560 - val_acc: 0.8322\n",
      "Epoch 129/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3508 - acc: 0.8374\n",
      "Epoch 00129: val_loss improved from 0.35592 to 0.35540, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3509 - acc: 0.8374 - val_loss: 0.3554 - val_acc: 0.8322\n",
      "Epoch 130/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3506 - acc: 0.8370\n",
      "Epoch 00130: val_loss did not improve from 0.35540\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3507 - acc: 0.8376 - val_loss: 0.3556 - val_acc: 0.8322\n",
      "Epoch 131/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3503 - acc: 0.8379\n",
      "Epoch 00131: val_loss did not improve from 0.35540\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3507 - acc: 0.8377 - val_loss: 0.3559 - val_acc: 0.8325\n",
      "Epoch 132/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3498 - acc: 0.8377\n",
      "Epoch 00132: val_loss did not improve from 0.35540\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3508 - acc: 0.8372 - val_loss: 0.3556 - val_acc: 0.8316\n",
      "Epoch 133/400\n",
      " 1000/37500 [..............................] - ETA: 0s - loss: 0.3432 - acc: 0.8390\n",
      "Epoch 00133: val_loss did not improve from 0.35540\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3507 - acc: 0.8373 - val_loss: 0.3558 - val_acc: 0.8314\n",
      "Epoch 134/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3512 - acc: 0.8362\n",
      "Epoch 00134: val_loss did not improve from 0.35540\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3522 - acc: 0.8359 - val_loss: 0.3555 - val_acc: 0.8314\n",
      "Epoch 135/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3504 - acc: 0.8374\n",
      "Epoch 00135: val_loss improved from 0.35540 to 0.35526, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3505 - acc: 0.8378 - val_loss: 0.3553 - val_acc: 0.8318\n",
      "Epoch 136/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3502 - acc: 0.8379\n",
      "Epoch 00136: val_loss did not improve from 0.35526\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3506 - acc: 0.8381 - val_loss: 0.3555 - val_acc: 0.8320\n",
      "Epoch 137/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3506 - acc: 0.8381\n",
      "Epoch 00137: val_loss did not improve from 0.35526\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3507 - acc: 0.8381 - val_loss: 0.3560 - val_acc: 0.8314\n",
      "Epoch 138/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3518 - acc: 0.8373\n",
      "Epoch 00138: val_loss did not improve from 0.35526\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3511 - acc: 0.8379 - val_loss: 0.3553 - val_acc: 0.8322\n",
      "Epoch 139/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3508 - acc: 0.8388\n",
      "Epoch 00139: val_loss did not improve from 0.35526\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3511 - acc: 0.8378 - val_loss: 0.3574 - val_acc: 0.8312\n",
      "Epoch 140/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3522 - acc: 0.8358\n",
      "Epoch 00140: val_loss did not improve from 0.35526\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3512 - acc: 0.8371 - val_loss: 0.3554 - val_acc: 0.8320\n",
      "Epoch 141/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3497 - acc: 0.8379\n",
      "Epoch 00141: val_loss did not improve from 0.35526\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3506 - acc: 0.8378 - val_loss: 0.3553 - val_acc: 0.8314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3510 - acc: 0.8369\n",
      "Epoch 00142: val_loss did not improve from 0.35526\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3507 - acc: 0.8372 - val_loss: 0.3560 - val_acc: 0.8318\n",
      "Epoch 143/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3514 - acc: 0.8374\n",
      "Epoch 00143: val_loss did not improve from 0.35526\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3509 - acc: 0.8377 - val_loss: 0.3560 - val_acc: 0.8321\n",
      "Epoch 144/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3500 - acc: 0.8371\n",
      "Epoch 00144: val_loss did not improve from 0.35526\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3508 - acc: 0.8376 - val_loss: 0.3565 - val_acc: 0.8318\n",
      "Epoch 145/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3576 - acc: 0.841 - ETA: 0s - loss: 0.3509 - acc: 0.8375\n",
      "Epoch 00145: val_loss did not improve from 0.35526\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3506 - acc: 0.8377 - val_loss: 0.3553 - val_acc: 0.8321\n",
      "Epoch 146/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3479 - acc: 0.8386\n",
      "Epoch 00146: val_loss improved from 0.35526 to 0.35496, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3502 - acc: 0.8378 - val_loss: 0.3550 - val_acc: 0.8317\n",
      "Epoch 147/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3499 - acc: 0.8375\n",
      "Epoch 00147: val_loss did not improve from 0.35496\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3503 - acc: 0.8376 - val_loss: 0.3553 - val_acc: 0.8322\n",
      "Epoch 148/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3522 - acc: 0.8362\n",
      "Epoch 00148: val_loss did not improve from 0.35496\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3502 - acc: 0.8379 - val_loss: 0.3551 - val_acc: 0.8319\n",
      "Epoch 149/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3503 - acc: 0.8383\n",
      "Epoch 00149: val_loss did not improve from 0.35496\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3503 - acc: 0.8380 - val_loss: 0.3551 - val_acc: 0.8327\n",
      "Epoch 150/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3498 - acc: 0.8374\n",
      "Epoch 00150: val_loss did not improve from 0.35496\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3501 - acc: 0.8377 - val_loss: 0.3551 - val_acc: 0.8321\n",
      "Epoch 151/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3478 - acc: 0.8397\n",
      "Epoch 00151: val_loss improved from 0.35496 to 0.35494, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 1s 26us/sample - loss: 0.3501 - acc: 0.8377 - val_loss: 0.3549 - val_acc: 0.8326\n",
      "Epoch 152/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3503 - acc: 0.8379\n",
      "Epoch 00152: val_loss did not improve from 0.35494\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3500 - acc: 0.8382 - val_loss: 0.3550 - val_acc: 0.8324\n",
      "Epoch 153/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3501 - acc: 0.8377\n",
      "Epoch 00153: val_loss did not improve from 0.35494\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3502 - acc: 0.8381 - val_loss: 0.3552 - val_acc: 0.8320\n",
      "Epoch 154/400\n",
      "30000/37500 [=======================>......] - ETA: 0s - loss: 0.3515 - acc: 0.8385\n",
      "Epoch 00154: val_loss did not improve from 0.35494\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3502 - acc: 0.8379 - val_loss: 0.3550 - val_acc: 0.8327\n",
      "Epoch 155/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3494 - acc: 0.8385\n",
      "Epoch 00155: val_loss did not improve from 0.35494\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3501 - acc: 0.8381 - val_loss: 0.3551 - val_acc: 0.8315\n",
      "Epoch 156/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3503 - acc: 0.8384\n",
      "Epoch 00156: val_loss did not improve from 0.35494\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3500 - acc: 0.8381 - val_loss: 0.3550 - val_acc: 0.8321\n",
      "Epoch 157/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3501 - acc: 0.8389\n",
      "Epoch 00157: val_loss did not improve from 0.35494\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3498 - acc: 0.8386 - val_loss: 0.3550 - val_acc: 0.8318\n",
      "Epoch 158/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3498 - acc: 0.8379\n",
      "Epoch 00158: val_loss improved from 0.35494 to 0.35486, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3498 - acc: 0.8381 - val_loss: 0.3549 - val_acc: 0.8325\n",
      "Epoch 159/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3509 - acc: 0.8371\n",
      "Epoch 00159: val_loss improved from 0.35486 to 0.35483, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3498 - acc: 0.8379 - val_loss: 0.3548 - val_acc: 0.8324\n",
      "Epoch 160/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3498 - acc: 0.8380\n",
      "Epoch 00160: val_loss improved from 0.35483 to 0.35482, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3497 - acc: 0.8381 - val_loss: 0.3548 - val_acc: 0.8324\n",
      "Epoch 161/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3497 - acc: 0.8385\n",
      "Epoch 00161: val_loss did not improve from 0.35482\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3498 - acc: 0.8380 - val_loss: 0.3549 - val_acc: 0.8327\n",
      "Epoch 162/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3497 - acc: 0.8380\n",
      "Epoch 00162: val_loss did not improve from 0.35482\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3498 - acc: 0.8383 - val_loss: 0.3548 - val_acc: 0.8322\n",
      "Epoch 163/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3499 - acc: 0.8378\n",
      "Epoch 00163: val_loss did not improve from 0.35482\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3497 - acc: 0.8382 - val_loss: 0.3549 - val_acc: 0.8322\n",
      "Epoch 164/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3493 - acc: 0.8381\n",
      "Epoch 00164: val_loss did not improve from 0.35482\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3497 - acc: 0.8379 - val_loss: 0.3549 - val_acc: 0.8322\n",
      "Epoch 165/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3495 - acc: 0.8382\n",
      "Epoch 00165: val_loss did not improve from 0.35482\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3498 - acc: 0.8378 - val_loss: 0.3551 - val_acc: 0.8320\n",
      "Epoch 166/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3505 - acc: 0.8369\n",
      "Epoch 00166: val_loss did not improve from 0.35482\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3498 - acc: 0.8382 - val_loss: 0.3549 - val_acc: 0.8322\n",
      "Epoch 167/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3489 - acc: 0.8378\n",
      "Epoch 00167: val_loss improved from 0.35482 to 0.35479, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3498 - acc: 0.8379 - val_loss: 0.3548 - val_acc: 0.8322\n",
      "Epoch 168/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3493 - acc: 0.8383\n",
      "Epoch 00168: val_loss improved from 0.35479 to 0.35475, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3497 - acc: 0.8382 - val_loss: 0.3548 - val_acc: 0.8329\n",
      "Epoch 169/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.8382\n",
      "Epoch 00169: val_loss did not improve from 0.35475\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3498 - acc: 0.8382 - val_loss: 0.3548 - val_acc: 0.8326\n",
      "Epoch 170/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3496 - acc: 0.8379\n",
      "Epoch 00170: val_loss did not improve from 0.35475\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3497 - acc: 0.8378 - val_loss: 0.3548 - val_acc: 0.8322\n",
      "Epoch 171/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3494 - acc: 0.8382\n",
      "Epoch 00171: val_loss did not improve from 0.35475\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3498 - acc: 0.8380 - val_loss: 0.3548 - val_acc: 0.8326\n",
      "Epoch 172/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3487 - acc: 0.8381\n",
      "Epoch 00172: val_loss improved from 0.35475 to 0.35475, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3497 - acc: 0.8380 - val_loss: 0.3548 - val_acc: 0.8327\n",
      "Epoch 173/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.8379\n",
      "Epoch 00173: val_loss did not improve from 0.35475\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3497 - acc: 0.8380 - val_loss: 0.3548 - val_acc: 0.8326\n",
      "Epoch 174/400\n",
      "28000/37500 [=====================>........] - ETA: 0s - loss: 0.3494 - acc: 0.8384\n",
      "Epoch 00174: val_loss did not improve from 0.35475\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3497 - acc: 0.8379 - val_loss: 0.3548 - val_acc: 0.8326\n",
      "Epoch 175/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3501 - acc: 0.8373\n",
      "Epoch 00175: val_loss did not improve from 0.35475\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3497 - acc: 0.8378 - val_loss: 0.3550 - val_acc: 0.8322\n",
      "Epoch 176/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3506 - acc: 0.8377\n",
      "Epoch 00176: val_loss did not improve from 0.35475\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3499 - acc: 0.8380 - val_loss: 0.3548 - val_acc: 0.8322\n",
      "Epoch 177/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3500 - acc: 0.8384\n",
      "Epoch 00177: val_loss did not improve from 0.35475\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3498 - acc: 0.8377 - val_loss: 0.3550 - val_acc: 0.8316\n",
      "Epoch 178/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3488 - acc: 0.8384\n",
      "Epoch 00178: val_loss did not improve from 0.35475\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3497 - acc: 0.8377 - val_loss: 0.3548 - val_acc: 0.8323\n",
      "Epoch 179/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3499 - acc: 0.8373\n",
      "Epoch 00179: val_loss improved from 0.35475 to 0.35474, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3496 - acc: 0.8377 - val_loss: 0.3547 - val_acc: 0.8323\n",
      "Epoch 180/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3496 - acc: 0.8380\n",
      "Epoch 00180: val_loss did not improve from 0.35474\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3496 - acc: 0.8381 - val_loss: 0.3548 - val_acc: 0.8321\n",
      "Epoch 181/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3496 - acc: 0.8379\n",
      "Epoch 00181: val_loss did not improve from 0.35474\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3495 - acc: 0.8378 - val_loss: 0.3548 - val_acc: 0.8320\n",
      "Epoch 182/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3505 - acc: 0.8370\n",
      "Epoch 00182: val_loss improved from 0.35474 to 0.35467, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3495 - acc: 0.8376 - val_loss: 0.3547 - val_acc: 0.8330\n",
      "Epoch 183/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3501 - acc: 0.8375\n",
      "Epoch 00183: val_loss improved from 0.35467 to 0.35464, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3495 - acc: 0.8378 - val_loss: 0.3546 - val_acc: 0.8326\n",
      "Epoch 184/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3483 - acc: 0.8388\n",
      "Epoch 00184: val_loss did not improve from 0.35464\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3495 - acc: 0.8381 - val_loss: 0.3548 - val_acc: 0.8322\n",
      "Epoch 185/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3486 - acc: 0.8385\n",
      "Epoch 00185: val_loss did not improve from 0.35464\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3495 - acc: 0.8381 - val_loss: 0.3547 - val_acc: 0.8330\n",
      "Epoch 186/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3497 - acc: 0.8381\n",
      "Epoch 00186: val_loss did not improve from 0.35464\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3495 - acc: 0.8383 - val_loss: 0.3546 - val_acc: 0.8331\n",
      "Epoch 187/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3499 - acc: 0.8373\n",
      "Epoch 00187: val_loss improved from 0.35464 to 0.35464, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3495 - acc: 0.8379 - val_loss: 0.3546 - val_acc: 0.8328\n",
      "Epoch 188/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3501 - acc: 0.8379\n",
      "Epoch 00188: val_loss did not improve from 0.35464\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3496 - acc: 0.8379 - val_loss: 0.3548 - val_acc: 0.8325\n",
      "Epoch 189/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3495 - acc: 0.8388\n",
      "Epoch 00189: val_loss did not improve from 0.35464\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3495 - acc: 0.8383 - val_loss: 0.3547 - val_acc: 0.8326\n",
      "Epoch 190/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3488 - acc: 0.8394\n",
      "Epoch 00190: val_loss did not improve from 0.35464\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3495 - acc: 0.8384 - val_loss: 0.3547 - val_acc: 0.8324\n",
      "Epoch 191/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3485 - acc: 0.8385\n",
      "Epoch 00191: val_loss did not improve from 0.35464\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3496 - acc: 0.8384 - val_loss: 0.3550 - val_acc: 0.8319\n",
      "Epoch 192/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3504 - acc: 0.8376\n",
      "Epoch 00192: val_loss improved from 0.35464 to 0.35463, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3496 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8330\n",
      "Epoch 193/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3492 - acc: 0.8380\n",
      "Epoch 00193: val_loss did not improve from 0.35463\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3495 - acc: 0.8380 - val_loss: 0.3547 - val_acc: 0.8321\n",
      "Epoch 194/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3487 - acc: 0.8377\n",
      "Epoch 00194: val_loss improved from 0.35463 to 0.35463, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3495 - acc: 0.8378 - val_loss: 0.3546 - val_acc: 0.8325\n",
      "Epoch 195/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3504 - acc: 0.8380\n",
      "Epoch 00195: val_loss improved from 0.35463 to 0.35462, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8383 - val_loss: 0.3546 - val_acc: 0.8326\n",
      "Epoch 196/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3495 - acc: 0.8384\n",
      "Epoch 00196: val_loss did not improve from 0.35462\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8383 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 197/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3497 - acc: 0.8375\n",
      "Epoch 00197: val_loss did not improve from 0.35462\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8381 - val_loss: 0.3547 - val_acc: 0.8319\n",
      "Epoch 198/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3501 - acc: 0.8376\n",
      "Epoch 00198: val_loss did not improve from 0.35462\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8379 - val_loss: 0.3546 - val_acc: 0.8321\n",
      "Epoch 199/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3497 - acc: 0.8380\n",
      "Epoch 00199: val_loss improved from 0.35462 to 0.35462, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8380 - val_loss: 0.3546 - val_acc: 0.8326\n",
      "Epoch 200/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3501 - acc: 0.8379\n",
      "Epoch 00200: val_loss did not improve from 0.35462\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8321\n",
      "Epoch 201/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3497 - acc: 0.8383\n",
      "Epoch 00201: val_loss did not improve from 0.35462\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8382 - val_loss: 0.3547 - val_acc: 0.8321\n",
      "Epoch 202/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3495 - acc: 0.8379\n",
      "Epoch 00202: val_loss improved from 0.35462 to 0.35461, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8325\n",
      "Epoch 203/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3500 - acc: 0.8377\n",
      "Epoch 00203: val_loss did not improve from 0.35461\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8381 - val_loss: 0.3547 - val_acc: 0.8322\n",
      "Epoch 204/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3490 - acc: 0.8383\n",
      "Epoch 00204: val_loss improved from 0.35461 to 0.35460, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8326\n",
      "Epoch 205/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3500 - acc: 0.8371\n",
      "Epoch 00205: val_loss did not improve from 0.35460\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8322\n",
      "Epoch 206/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3506 - acc: 0.8370\n",
      "Epoch 00206: val_loss did not improve from 0.35460\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8321\n",
      "Epoch 207/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3494 - acc: 0.8386\n",
      "Epoch 00207: val_loss did not improve from 0.35460\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8321\n",
      "Epoch 208/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3481 - acc: 0.8388\n",
      "Epoch 00208: val_loss did not improve from 0.35460\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 209/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3498 - acc: 0.8381\n",
      "Epoch 00209: val_loss did not improve from 0.35460\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8321\n",
      "Epoch 210/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3489 - acc: 0.8382\n",
      "Epoch 00210: val_loss did not improve from 0.35460\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 211/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3494 - acc: 0.8381\n",
      "Epoch 00211: val_loss did not improve from 0.35460\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 212/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3496 - acc: 0.8382\n",
      "Epoch 00212: val_loss did not improve from 0.35460\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8383 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 213/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3497 - acc: 0.8382\n",
      "Epoch 00213: val_loss did not improve from 0.35460\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8380 - val_loss: 0.3546 - val_acc: 0.8321\n",
      "Epoch 214/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.8381\n",
      "Epoch 00214: val_loss did not improve from 0.35460\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8321\n",
      "Epoch 215/400\n",
      " 1000/37500 [..............................] - ETA: 0s - loss: 0.3422 - acc: 0.8510\n",
      "Epoch 00215: val_loss did not improve from 0.35460\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8322\n",
      "Epoch 216/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3492 - acc: 0.8384\n",
      "Epoch 00216: val_loss did not improve from 0.35460\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8322\n",
      "Epoch 217/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3516 - acc: 0.8368\n",
      "Epoch 00217: val_loss improved from 0.35460 to 0.35459, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 218/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3495 - acc: 0.8384\n",
      "Epoch 00218: val_loss did not improve from 0.35459\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 219/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3470 - acc: 0.8395\n",
      "Epoch 00219: val_loss did not improve from 0.35459\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 220/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3493 - acc: 0.8372\n",
      "Epoch 00220: val_loss did not improve from 0.35459\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 221/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3502 - acc: 0.8378\n",
      "Epoch 00221: val_loss did not improve from 0.35459\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 222/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3496 - acc: 0.8381\n",
      "Epoch 00222: val_loss did not improve from 0.35459\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 223/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3484 - acc: 0.8392\n",
      "Epoch 00223: val_loss did not improve from 0.35459\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3494 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8322\n",
      "Epoch 224/400\n",
      "28000/37500 [=====================>........] - ETA: 0s - loss: 0.3515 - acc: 0.8366\n",
      "Epoch 00224: val_loss improved from 0.35459 to 0.35459, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 225/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3501 - acc: 0.8382\n",
      "Epoch 00225: val_loss did not improve from 0.35459\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 226/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3480 - acc: 0.8387\n",
      "Epoch 00226: val_loss did not improve from 0.35459\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3502 - acc: 0.8380\n",
      "Epoch 00227: val_loss did not improve from 0.35459\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8322\n",
      "Epoch 228/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3492 - acc: 0.8380\n",
      "Epoch 00228: val_loss did not improve from 0.35459\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8383 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 229/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3492 - acc: 0.8383\n",
      "Epoch 00229: val_loss did not improve from 0.35459\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8322\n",
      "Epoch 230/400\n",
      " 1000/37500 [..............................] - ETA: 0s - loss: 0.3507 - acc: 0.8300\n",
      "Epoch 00230: val_loss improved from 0.35459 to 0.35458, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 231/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3500 - acc: 0.8383\n",
      "Epoch 00231: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 232/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3495 - acc: 0.8376\n",
      "Epoch 00232: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 233/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3493 - acc: 0.8380\n",
      "Epoch 00233: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8322\n",
      "Epoch 234/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3490 - acc: 0.8383\n",
      "Epoch 00234: val_loss improved from 0.35458 to 0.35458, saving model to models_chpt/best_modelD.hdf5\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 235/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3506 - acc: 0.8376\n",
      "Epoch 00235: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 236/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3495 - acc: 0.8379\n",
      "Epoch 00236: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 237/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3494 - acc: 0.8385\n",
      "Epoch 00237: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 238/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3495 - acc: 0.8382\n",
      "Epoch 00238: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 239/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3499 - acc: 0.8374\n",
      "Epoch 00239: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 240/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3487 - acc: 0.8386\n",
      "Epoch 00240: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 241/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3507 - acc: 0.8372\n",
      "Epoch 00241: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8322\n",
      "Epoch 242/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3492 - acc: 0.8380\n",
      "Epoch 00242: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 243/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3483 - acc: 0.8386\n",
      "Epoch 00243: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 244/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3491 - acc: 0.8381\n",
      "Epoch 00244: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8322\n",
      "Epoch 245/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3496 - acc: 0.8381\n",
      "Epoch 00245: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8322\n",
      "Epoch 246/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3496 - acc: 0.8383\n",
      "Epoch 00246: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 247/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3213 - acc: 0.852 - ETA: 0s - loss: 0.3498 - acc: 0.8382\n",
      "Epoch 00247: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 248/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3495 - acc: 0.8377\n",
      "Epoch 00248: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 249/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3491 - acc: 0.8384\n",
      "Epoch 00249: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 250/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3490 - acc: 0.8382\n",
      "Epoch 00250: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 251/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3498 - acc: 0.8380\n",
      "Epoch 00251: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 252/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3499 - acc: 0.8379\n",
      "Epoch 00252: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 253/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3500 - acc: 0.8378\n",
      "Epoch 00253: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 254/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3493 - acc: 0.8390\n",
      "Epoch 00254: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 255/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3501 - acc: 0.8374\n",
      "Epoch 00255: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 256/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3500 - acc: 0.8379\n",
      "Epoch 00256: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 257/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3505 - acc: 0.8376\n",
      "Epoch 00257: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 258/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3491 - acc: 0.8382\n",
      "Epoch 00258: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 259/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3491 - acc: 0.8386\n",
      "Epoch 00259: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 260/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3484 - acc: 0.8386\n",
      "Epoch 00260: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 261/400\n",
      "28000/37500 [=====================>........] - ETA: 0s - loss: 0.3532 - acc: 0.8366\n",
      "Epoch 00261: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 262/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3489 - acc: 0.8381\n",
      "Epoch 00262: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 263/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3485 - acc: 0.8394\n",
      "Epoch 00263: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8382 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 264/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3493 - acc: 0.8377\n",
      "Epoch 00264: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 265/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3498 - acc: 0.8381\n",
      "Epoch 00265: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 266/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3484 - acc: 0.8388\n",
      "Epoch 00266: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 267/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3498 - acc: 0.8380\n",
      "Epoch 00267: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 268/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3472 - acc: 0.8388\n",
      "Epoch 00268: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 269/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3488 - acc: 0.8382\n",
      "Epoch 00269: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 270/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3498 - acc: 0.8377\n",
      "Epoch 00270: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 271/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3489 - acc: 0.8386\n",
      "Epoch 00271: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 272/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3496 - acc: 0.8384\n",
      "Epoch 00272: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 273/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3495 - acc: 0.8380\n",
      "Epoch 00273: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 274/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3495 - acc: 0.8378\n",
      "Epoch 00274: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 275/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3494 - acc: 0.8377\n",
      "Epoch 00275: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 276/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3491 - acc: 0.8384\n",
      "Epoch 00276: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 277/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3501 - acc: 0.8376\n",
      "Epoch 00277: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 278/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3493 - acc: 0.8384\n",
      "Epoch 00278: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 279/400\n",
      " 1000/37500 [..............................] - ETA: 0s - loss: 0.3700 - acc: 0.8270\n",
      "Epoch 00279: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 280/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.8385\n",
      "Epoch 00280: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 281/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3497 - acc: 0.8380\n",
      "Epoch 00281: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 282/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3504 - acc: 0.8372\n",
      "Epoch 00282: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 283/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3499 - acc: 0.8380\n",
      "Epoch 00283: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 284/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3510 - acc: 0.8366\n",
      "Epoch 00284: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 285/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3503 - acc: 0.8377\n",
      "Epoch 00285: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3485 - acc: 0.8385\n",
      "Epoch 00286: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 287/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3499 - acc: 0.8379\n",
      "Epoch 00287: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 288/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3487 - acc: 0.8382\n",
      "Epoch 00288: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 289/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3479 - acc: 0.8390\n",
      "Epoch 00289: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 290/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3503 - acc: 0.8373\n",
      "Epoch 00290: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 291/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3490 - acc: 0.8375\n",
      "Epoch 00291: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 292/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3507 - acc: 0.8376\n",
      "Epoch 00292: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8323\n",
      "Epoch 293/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3488 - acc: 0.8384\n",
      "Epoch 00293: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 294/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3505 - acc: 0.8370\n",
      "Epoch 00294: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 295/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3496 - acc: 0.8380\n",
      "Epoch 00295: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 296/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3490 - acc: 0.8382\n",
      "Epoch 00296: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 297/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3493 - acc: 0.8381\n",
      "Epoch 00297: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 298/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3479 - acc: 0.8396\n",
      "Epoch 00298: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 299/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3505 - acc: 0.8376\n",
      "Epoch 00299: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 300/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3497 - acc: 0.8382\n",
      "Epoch 00300: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 301/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3499 - acc: 0.8375\n",
      "Epoch 00301: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 302/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3492 - acc: 0.8377\n",
      "Epoch 00302: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 303/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3493 - acc: 0.8382\n",
      "Epoch 00303: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 304/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3500 - acc: 0.8376\n",
      "Epoch 00304: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 305/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3495 - acc: 0.8380\n",
      "Epoch 00305: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 306/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3496 - acc: 0.8381\n",
      "Epoch 00306: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 307/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3489 - acc: 0.8391\n",
      "Epoch 00307: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 308/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.8380\n",
      "Epoch 00308: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 309/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3486 - acc: 0.8385\n",
      "Epoch 00309: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 310/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3488 - acc: 0.8384\n",
      "Epoch 00310: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 311/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3488 - acc: 0.8386\n",
      "Epoch 00311: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 312/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3408 - acc: 0.850 - ETA: 0s - loss: 0.3488 - acc: 0.8382\n",
      "Epoch 00312: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 313/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3482 - acc: 0.8385\n",
      "Epoch 00313: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 314/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3490 - acc: 0.8383\n",
      "Epoch 00314: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 315/400\n",
      " 1000/37500 [..............................] - ETA: 0s - loss: 0.3439 - acc: 0.8520\n",
      "Epoch 00315: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 316/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3501 - acc: 0.8375\n",
      "Epoch 00316: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 317/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3497 - acc: 0.8381\n",
      "Epoch 00317: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 318/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.8384\n",
      "Epoch 00318: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 319/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3500 - acc: 0.8377\n",
      "Epoch 00319: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 320/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8382\n",
      "Epoch 00320: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 321/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3499 - acc: 0.8378\n",
      "Epoch 00321: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 322/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3492 - acc: 0.8383\n",
      "Epoch 00322: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 323/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3487 - acc: 0.8388\n",
      "Epoch 00323: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 324/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.8382\n",
      "Epoch 00324: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 325/400\n",
      " 1000/37500 [..............................] - ETA: 0s - loss: 0.3716 - acc: 0.8320\n",
      "Epoch 00325: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 326/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3490 - acc: 0.8384\n",
      "Epoch 00326: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 327/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3498 - acc: 0.8381\n",
      "Epoch 00327: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 328/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3502 - acc: 0.8382\n",
      "Epoch 00328: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 329/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3487 - acc: 0.8385\n",
      "Epoch 00329: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 330/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3487 - acc: 0.8385\n",
      "Epoch 00330: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 331/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3498 - acc: 0.8376\n",
      "Epoch 00331: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 332/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8381\n",
      "Epoch 00332: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 333/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3497 - acc: 0.8377\n",
      "Epoch 00333: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 334/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3502 - acc: 0.8376\n",
      "Epoch 00334: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 335/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3501 - acc: 0.8378\n",
      "Epoch 00335: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 336/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3482 - acc: 0.8386\n",
      "Epoch 00336: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 337/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3499 - acc: 0.8381\n",
      "Epoch 00337: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 338/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3498 - acc: 0.8380\n",
      "Epoch 00338: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 339/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3492 - acc: 0.8381\n",
      "Epoch 00339: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 340/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3496 - acc: 0.8379\n",
      "Epoch 00340: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 341/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3492 - acc: 0.8386\n",
      "Epoch 00341: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 342/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3494 - acc: 0.8387\n",
      "Epoch 00342: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 343/400\n",
      "30000/37500 [=======================>......] - ETA: 0s - loss: 0.3483 - acc: 0.8393\n",
      "Epoch 00343: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 344/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3501 - acc: 0.8380\n",
      "Epoch 00344: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 345/400\n",
      "29000/37500 [======================>.......] - ETA: 0s - loss: 0.3493 - acc: 0.8380\n",
      "Epoch 00345: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 346/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3480 - acc: 0.8393\n",
      "Epoch 00346: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 347/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3494 - acc: 0.8382\n",
      "Epoch 00347: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 348/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3492 - acc: 0.8381\n",
      "Epoch 00348: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 349/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3493 - acc: 0.8382\n",
      "Epoch 00349: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 350/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3498 - acc: 0.8379\n",
      "Epoch 00350: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 351/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3483 - acc: 0.8390\n",
      "Epoch 00351: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 352/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3492 - acc: 0.8381\n",
      "Epoch 00352: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 353/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3477 - acc: 0.8391\n",
      "Epoch 00353: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 354/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3482 - acc: 0.8388\n",
      "Epoch 00354: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 355/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3489 - acc: 0.8389\n",
      "Epoch 00355: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 356/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3517 - acc: 0.8374\n",
      "Epoch 00356: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 357/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3484 - acc: 0.8392\n",
      "Epoch 00357: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 358/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3481 - acc: 0.8384\n",
      "Epoch 00358: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 359/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3485 - acc: 0.8386\n",
      "Epoch 00359: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 360/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3501 - acc: 0.8375\n",
      "Epoch 00360: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 361/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3498 - acc: 0.8378\n",
      "Epoch 00361: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 362/400\n",
      "35000/37500 [===========================>..] - ETA: 0s - loss: 0.3488 - acc: 0.8384\n",
      "Epoch 00362: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 363/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3491 - acc: 0.8388\n",
      "Epoch 00363: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 364/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3499 - acc: 0.8375\n",
      "Epoch 00364: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 365/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3499 - acc: 0.8378\n",
      "Epoch 00365: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 366/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3482 - acc: 0.8386\n",
      "Epoch 00366: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 367/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3497 - acc: 0.8378\n",
      "Epoch 00367: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 368/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3486 - acc: 0.8384\n",
      "Epoch 00368: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 369/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3495 - acc: 0.8380\n",
      "Epoch 00369: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 370/400\n",
      " 1000/37500 [..............................] - ETA: 0s - loss: 0.3510 - acc: 0.8350\n",
      "Epoch 00370: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 371/400\n",
      "36000/37500 [===========================>..] - ETA: 0s - loss: 0.3479 - acc: 0.8388\n",
      "Epoch 00371: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 372/400\n",
      "34000/37500 [==========================>...] - ETA: 0s - loss: 0.3507 - acc: 0.8374\n",
      "Epoch 00372: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 373/400\n",
      "37000/37500 [============================>.] - ETA: 0s - loss: 0.3498 - acc: 0.8379\n",
      "Epoch 00373: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 374/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3494 - acc: 0.8384\n",
      "Epoch 00374: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 375/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3490 - acc: 0.8383\n",
      "Epoch 00375: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 376/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3506 - acc: 0.8370\n",
      "Epoch 00376: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 377/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3503 - acc: 0.8369\n",
      "Epoch 00377: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 378/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3488 - acc: 0.8385\n",
      "Epoch 00378: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 379/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3505 - acc: 0.8367\n",
      "Epoch 00379: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 380/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3503 - acc: 0.8377\n",
      "Epoch 00380: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 381/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3498 - acc: 0.8387\n",
      "Epoch 00381: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 382/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3495 - acc: 0.8377\n",
      "Epoch 00382: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 383/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3482 - acc: 0.8380\n",
      "Epoch 00383: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 384/400\n",
      "27000/37500 [====================>.........] - ETA: 0s - loss: 0.3507 - acc: 0.8376\n",
      "Epoch 00384: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 385/400\n",
      "24000/37500 [==================>...........] - ETA: 0s - loss: 0.3500 - acc: 0.8378\n",
      "Epoch 00385: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 386/400\n",
      "27000/37500 [====================>.........] - ETA: 0s - loss: 0.3503 - acc: 0.8371\n",
      "Epoch 00386: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 387/400\n",
      "28000/37500 [=====================>........] - ETA: 0s - loss: 0.3469 - acc: 0.8393\n",
      "Epoch 00387: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 388/400\n",
      "30000/37500 [=======================>......] - ETA: 0s - loss: 0.3482 - acc: 0.8386\n",
      "Epoch 00388: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 389/400\n",
      "26000/37500 [===================>..........] - ETA: 0s - loss: 0.3475 - acc: 0.8391\n",
      "Epoch 00389: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 390/400\n",
      "30000/37500 [=======================>......] - ETA: 0s - loss: 0.3500 - acc: 0.8376\n",
      "Epoch 00390: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 391/400\n",
      "32000/37500 [========================>.....] - ETA: 0s - loss: 0.3481 - acc: 0.8382\n",
      "Epoch 00391: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 392/400\n",
      "28000/37500 [=====================>........] - ETA: 0s - loss: 0.3489 - acc: 0.8389\n",
      "Epoch 00392: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 393/400\n",
      "29000/37500 [======================>.......] - ETA: 0s - loss: 0.3507 - acc: 0.8369\n",
      "Epoch 00393: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 394/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3502 - acc: 0.8375\n",
      "Epoch 00394: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 395/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3517 - acc: 0.8372\n",
      "Epoch 00395: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 396/400\n",
      "33000/37500 [=========================>....] - ETA: 0s - loss: 0.3497 - acc: 0.8383\n",
      "Epoch 00396: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 397/400\n",
      "25000/37500 [===================>..........] - ETA: 0s - loss: 0.3470 - acc: 0.8411\n",
      "Epoch 00397: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 3us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 398/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3487 - acc: 0.8391\n",
      "Epoch 00398: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 399/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3469 - acc: 0.8394\n",
      "Epoch 00399: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n",
      "Epoch 400/400\n",
      "31000/37500 [=======================>......] - ETA: 0s - loss: 0.3481 - acc: 0.8392\n",
      "Epoch 00400: val_loss did not improve from 0.35458\n",
      "37500/37500 [==============================] - 0s 2us/sample - loss: 0.3493 - acc: 0.8381 - val_loss: 0.3546 - val_acc: 0.8324\n"
     ]
    }
   ],
   "source": [
    "bz = 1000\n",
    "epochs = 400\n",
    "history = model.fit(\n",
    "    xtrain,\n",
    "    ytrain,\n",
    "    epochs = epochs,\n",
    "    batch_size = bz,\n",
    "    validation_data =(xtest, ytest),\n",
    "    callbacks = [mcp, red_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss Vs # of epochs')"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "plt.plot( [i for i in range(epochs)], history.history['val_loss'], label = \"validation loss\")\n",
    "plt.xlabel(\"#of epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Loss Vs # of epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss Vs # of epochs')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(epochs)], history.history['loss'], label = \"Train loss\")\n",
    "plt.xlabel(\"#of epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Loss Vs # of epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Learning rate Vs # of epochs')"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(epochs)], history.history['lr'], label = \"Train loss\")\n",
    "plt.xlabel(\"#of epochs\")\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.title(\"Learning rate Vs # of epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500/12500 [==============================] - 0s 15us/sample - loss: 0.3546 - acc: 0.8324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3545808448839188, 0.8324]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc', 'lr'])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best.load_weights(\"models_chpt/best_modelD.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500/12500 [==============================] - 0s 12us/sample - loss: 0.3546 - acc: 0.8323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.35458033203601835, 0.83232]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_best.evaluate(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision1",
   "language": "python",
   "name": "vision1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
