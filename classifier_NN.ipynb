{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from VisionUtils import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectFpr, mutual_info_classif\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat1 = np.load(\"features/feat1.npy\")\n",
    "feat2 = np.load(\"features/feat2.npy\")\n",
    "labels = np.load(\"features/labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(feat1, feat2):\n",
    "    f1 = [feat1[i1].reshape(-1,1) for i1 in range(len(feat1))]\n",
    "    f2 = [feat2[i2].reshape(-1,1) for i2 in range(len(feat2))]\n",
    "    cos_d = np.array([feat_distance_cosine_scalar(f1[i].T, f2[i]) for i in range(len(feat1))])\n",
    "    cos_d = cos_d.reshape(-1,1)\n",
    "    sqr_diff = np.power(np.abs(feat1- feat2), 2)\n",
    "    rat = feat1/feat2\n",
    "    data = np.hstack([cos_d, sqr_diff])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = trans(feat1, feat2)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(scaled_data, labels, \n",
    "                                                shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "he_init = tf.keras.initializers.VarianceScaling()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense( 128, activation = 'relu', kernel_initializer = he_init, \n",
    "                kernel_regularizer = l1(0.001), \n",
    "                #input_shape = (1,128)\n",
    "               ))\n",
    "model.add(Dense(32, activation = 'relu',kernel_initializer = he_init))\n",
    "model.add(Dense(1, activation = \"tanh\", kernel_initializer = he_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "mcp = ModelCheckpoint(\"models_chpt/best_modelD.hdf5\",verbose = 1, \n",
    "                      monitor = \"val_loss\", save_best_only = True, save_weights_only = True)\n",
    "red_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor =0.5)\n",
    "opt = tf.keras.optimizers.Adam(0.01)\n",
    "model.compile(optimizer = opt, loss = \"binary_crossentropy\", \n",
    "              metrics = [\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0819 13:03:09.438328  9756 deprecation.py:323] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21450 samples, validate on 7150 samples\n",
      "Epoch 1/400\n",
      "18000/21450 [========================>.....] - ETA: 0s - loss: 2.0038 - acc: 0.5011\n",
      "Epoch 00001: val_loss improved from inf to 1.32453, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 17us/sample - loss: 1.9097 - acc: 0.5443 - val_loss: 1.3245 - val_acc: 0.8860\n",
      "Epoch 2/400\n",
      "21000/21450 [============================>.] - ETA: 0s - loss: 1.0395 - acc: 0.8788\n",
      "Epoch 00002: val_loss improved from 1.32453 to 0.80829, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 4us/sample - loss: 1.0348 - acc: 0.8790 - val_loss: 0.8083 - val_acc: 0.8961\n",
      "Epoch 3/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.8202 - acc: 0.8880\n",
      "Epoch 00003: val_loss improved from 0.80829 to 0.59163, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 4us/sample - loss: 0.7022 - acc: 0.8923 - val_loss: 0.5916 - val_acc: 0.8979\n",
      "Epoch 4/400\n",
      "21000/21450 [============================>.] - ETA: 0s - loss: 0.5333 - acc: 0.8909\n",
      "Epoch 00004: val_loss improved from 0.59163 to 0.46899, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 4us/sample - loss: 0.5323 - acc: 0.8905 - val_loss: 0.4690 - val_acc: 0.8979\n",
      "Epoch 5/400\n",
      "20000/21450 [==========================>...] - ETA: 0s - loss: 0.4398 - acc: 0.8916\n",
      "Epoch 00005: val_loss improved from 0.46899 to 0.39999, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 4us/sample - loss: 0.4373 - acc: 0.8924 - val_loss: 0.4000 - val_acc: 0.8959\n",
      "Epoch 6/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.4145 - acc: 0.8970\n",
      "Epoch 00006: val_loss improved from 0.39999 to 0.35815, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3866 - acc: 0.8904 - val_loss: 0.3582 - val_acc: 0.8961\n",
      "Epoch 7/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3751 - acc: 0.8910\n",
      "Epoch 00007: val_loss improved from 0.35815 to 0.33208, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3510 - acc: 0.8929 - val_loss: 0.3321 - val_acc: 0.8964\n",
      "Epoch 8/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3421 - acc: 0.8960\n",
      "Epoch 00008: val_loss improved from 0.33208 to 0.32821, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3339 - acc: 0.8916 - val_loss: 0.3282 - val_acc: 0.8930\n",
      "Epoch 9/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3561 - acc: 0.8870\n",
      "Epoch 00009: val_loss improved from 0.32821 to 0.30777, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3232 - acc: 0.8907 - val_loss: 0.3078 - val_acc: 0.8965\n",
      "Epoch 10/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3143 - acc: 0.8880\n",
      "Epoch 00010: val_loss improved from 0.30777 to 0.30466, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3111 - acc: 0.8928 - val_loss: 0.3047 - val_acc: 0.8952\n",
      "Epoch 11/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3114 - acc: 0.8890\n",
      "Epoch 00011: val_loss improved from 0.30466 to 0.30442, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.3065 - acc: 0.8914 - val_loss: 0.3044 - val_acc: 0.8929\n",
      "Epoch 12/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2958 - acc: 0.9010\n",
      "Epoch 00012: val_loss improved from 0.30442 to 0.29887, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 4us/sample - loss: 0.3083 - acc: 0.8897 - val_loss: 0.2989 - val_acc: 0.8945\n",
      "Epoch 13/400\n",
      "21000/21450 [============================>.] - ETA: 0s - loss: 0.2996 - acc: 0.8918\n",
      "Epoch 00013: val_loss improved from 0.29887 to 0.29524, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 4us/sample - loss: 0.2998 - acc: 0.8916 - val_loss: 0.2952 - val_acc: 0.8955\n",
      "Epoch 14/400\n",
      "20000/21450 [==========================>...] - ETA: 0s - loss: 0.2944 - acc: 0.8932\n",
      "Epoch 00014: val_loss improved from 0.29524 to 0.29260, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 4us/sample - loss: 0.2960 - acc: 0.8928 - val_loss: 0.2926 - val_acc: 0.8969\n",
      "Epoch 15/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3027 - acc: 0.8840\n",
      "Epoch 00015: val_loss did not improve from 0.29260\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2945 - acc: 0.8937 - val_loss: 0.2927 - val_acc: 0.8947\n",
      "Epoch 16/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2815 - acc: 0.9010\n",
      "Epoch 00016: val_loss did not improve from 0.29260\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2961 - acc: 0.8912 - val_loss: 0.2948 - val_acc: 0.8957\n",
      "Epoch 17/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2864 - acc: 0.8910\n",
      "Epoch 00017: val_loss improved from 0.29260 to 0.28697, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2973 - acc: 0.8906 - val_loss: 0.2870 - val_acc: 0.8968\n",
      "Epoch 18/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2933 - acc: 0.8890\n",
      "Epoch 00018: val_loss did not improve from 0.28697\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2937 - acc: 0.8931 - val_loss: 0.2919 - val_acc: 0.8945\n",
      "Epoch 19/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2955 - acc: 0.8880\n",
      "Epoch 00019: val_loss did not improve from 0.28697\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2928 - acc: 0.8935 - val_loss: 0.2887 - val_acc: 0.8972\n",
      "Epoch 20/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2924 - acc: 0.8960\n",
      "Epoch 00020: val_loss did not improve from 0.28697\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2923 - acc: 0.8936 - val_loss: 0.2912 - val_acc: 0.8934\n",
      "Epoch 21/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2605 - acc: 0.9110\n",
      "Epoch 00021: val_loss did not improve from 0.28697\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2928 - acc: 0.8917 - val_loss: 0.2880 - val_acc: 0.8957\n",
      "Epoch 22/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3032 - acc: 0.8860\n",
      "Epoch 00022: val_loss did not improve from 0.28697\n",
      "21450/21450 [==============================] - 0s 2us/sample - loss: 0.2904 - acc: 0.8927 - val_loss: 0.2883 - val_acc: 0.8966\n",
      "Epoch 23/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2761 - acc: 0.8970\n",
      "Epoch 00023: val_loss did not improve from 0.28697\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2894 - acc: 0.8937 - val_loss: 0.2879 - val_acc: 0.8954\n",
      "Epoch 24/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2683 - acc: 0.9010\n",
      "Epoch 00024: val_loss improved from 0.28697 to 0.28593, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2914 - acc: 0.8923 - val_loss: 0.2859 - val_acc: 0.8965\n",
      "Epoch 25/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2647 - acc: 0.8970\n",
      "Epoch 00025: val_loss did not improve from 0.28593\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2915 - acc: 0.8916 - val_loss: 0.2918 - val_acc: 0.8931\n",
      "Epoch 26/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2944 - acc: 0.8770\n",
      "Epoch 00026: val_loss did not improve from 0.28593\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2909 - acc: 0.8910 - val_loss: 0.2977 - val_acc: 0.8906\n",
      "Epoch 27/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2658 - acc: 0.9010\n",
      "Epoch 00027: val_loss did not improve from 0.28593\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2916 - acc: 0.8913 - val_loss: 0.2887 - val_acc: 0.8950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2847 - acc: 0.8980\n",
      "Epoch 00028: val_loss did not improve from 0.28593\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2967 - acc: 0.8926 - val_loss: 0.2989 - val_acc: 0.8929\n",
      "Epoch 29/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2715 - acc: 0.9070\n",
      "Epoch 00029: val_loss did not improve from 0.28593\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2896 - acc: 0.8917 - val_loss: 0.2861 - val_acc: 0.8950\n",
      "Epoch 30/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2911 - acc: 0.8860\n",
      "Epoch 00030: val_loss improved from 0.28593 to 0.28151, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2865 - acc: 0.8929 - val_loss: 0.2815 - val_acc: 0.8969\n",
      "Epoch 31/400\n",
      "19000/21450 [=========================>....] - ETA: 0s - loss: 0.2820 - acc: 0.8953\n",
      "Epoch 00031: val_loss did not improve from 0.28151\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2836 - acc: 0.8942 - val_loss: 0.2815 - val_acc: 0.8959\n",
      "Epoch 32/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2824 - acc: 0.8920\n",
      "Epoch 00032: val_loss did not improve from 0.28151\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2835 - acc: 0.8935 - val_loss: 0.2856 - val_acc: 0.8937\n",
      "Epoch 33/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2967 - acc: 0.8840\n",
      "Epoch 00033: val_loss did not improve from 0.28151\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2876 - acc: 0.8916 - val_loss: 0.2835 - val_acc: 0.8964\n",
      "Epoch 34/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2414 - acc: 0.9060\n",
      "Epoch 00034: val_loss improved from 0.28151 to 0.28030, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2857 - acc: 0.8931 - val_loss: 0.2803 - val_acc: 0.8973\n",
      "Epoch 35/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2754 - acc: 0.9030\n",
      "Epoch 00035: val_loss improved from 0.28030 to 0.27948, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2840 - acc: 0.8923 - val_loss: 0.2795 - val_acc: 0.8975\n",
      "Epoch 36/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2694 - acc: 0.8940\n",
      "Epoch 00036: val_loss did not improve from 0.27948\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2818 - acc: 0.8946 - val_loss: 0.2898 - val_acc: 0.8937\n",
      "Epoch 37/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2821 - acc: 0.9050\n",
      "Epoch 00037: val_loss did not improve from 0.27948\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2849 - acc: 0.8932 - val_loss: 0.2833 - val_acc: 0.8958\n",
      "Epoch 38/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2783 - acc: 0.9050\n",
      "Epoch 00038: val_loss did not improve from 0.27948\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2842 - acc: 0.8924 - val_loss: 0.2795 - val_acc: 0.8968\n",
      "Epoch 39/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2786 - acc: 0.9020\n",
      "Epoch 00039: val_loss did not improve from 0.27948\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2824 - acc: 0.8950 - val_loss: 0.2814 - val_acc: 0.8969\n",
      "Epoch 40/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2759 - acc: 0.8900\n",
      "Epoch 00040: val_loss did not improve from 0.27948\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2933 - acc: 0.8892 - val_loss: 0.2948 - val_acc: 0.8913\n",
      "Epoch 41/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3155 - acc: 0.8730\n",
      "Epoch 00041: val_loss did not improve from 0.27948\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2876 - acc: 0.8917 - val_loss: 0.2815 - val_acc: 0.8971\n",
      "Epoch 42/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2893 - acc: 0.8820\n",
      "Epoch 00042: val_loss did not improve from 0.27948\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2813 - acc: 0.8937 - val_loss: 0.2811 - val_acc: 0.8969\n",
      "Epoch 43/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2413 - acc: 0.9220\n",
      "Epoch 00043: val_loss did not improve from 0.27948\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2812 - acc: 0.8938 - val_loss: 0.2815 - val_acc: 0.8966\n",
      "Epoch 44/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2497 - acc: 0.9130\n",
      "Epoch 00044: val_loss did not improve from 0.27948\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2844 - acc: 0.8935 - val_loss: 0.2843 - val_acc: 0.8962\n",
      "Epoch 45/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2707 - acc: 0.8980\n",
      "Epoch 00045: val_loss did not improve from 0.27948\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2848 - acc: 0.8926 - val_loss: 0.2846 - val_acc: 0.8952\n",
      "Epoch 46/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2750 - acc: 0.8910\n",
      "Epoch 00046: val_loss improved from 0.27948 to 0.27174, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2728 - acc: 0.8947 - val_loss: 0.2717 - val_acc: 0.8966\n",
      "Epoch 47/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3008 - acc: 0.8930\n",
      "Epoch 00047: val_loss improved from 0.27174 to 0.27024, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2697 - acc: 0.8950 - val_loss: 0.2702 - val_acc: 0.8980\n",
      "Epoch 48/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2532 - acc: 0.8980\n",
      "Epoch 00048: val_loss improved from 0.27024 to 0.26976, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2699 - acc: 0.8950 - val_loss: 0.2698 - val_acc: 0.8966\n",
      "Epoch 49/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2849 - acc: 0.8900\n",
      "Epoch 00049: val_loss improved from 0.26976 to 0.26937, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2692 - acc: 0.8952 - val_loss: 0.2694 - val_acc: 0.8971\n",
      "Epoch 50/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2671 - acc: 0.8930\n",
      "Epoch 00050: val_loss did not improve from 0.26937\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2701 - acc: 0.8948 - val_loss: 0.2717 - val_acc: 0.8955\n",
      "Epoch 51/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2654 - acc: 0.8920\n",
      "Epoch 00051: val_loss improved from 0.26937 to 0.26890, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2687 - acc: 0.8953 - val_loss: 0.2689 - val_acc: 0.8959\n",
      "Epoch 52/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2610 - acc: 0.9030\n",
      "Epoch 00052: val_loss did not improve from 0.26890\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2699 - acc: 0.8955 - val_loss: 0.2718 - val_acc: 0.8968\n",
      "Epoch 53/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2614 - acc: 0.9060\n",
      "Epoch 00053: val_loss did not improve from 0.26890\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2697 - acc: 0.8941 - val_loss: 0.2727 - val_acc: 0.8950\n",
      "Epoch 54/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2769 - acc: 0.8860\n",
      "Epoch 00054: val_loss did not improve from 0.26890\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2690 - acc: 0.8942 - val_loss: 0.2698 - val_acc: 0.8976\n",
      "Epoch 55/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2913 - acc: 0.8880\n",
      "Epoch 00055: val_loss did not improve from 0.26890\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2710 - acc: 0.8931 - val_loss: 0.2706 - val_acc: 0.8986\n",
      "Epoch 56/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2428 - acc: 0.9080\n",
      "Epoch 00056: val_loss did not improve from 0.26890\n",
      "21450/21450 [==============================] - 0s 2us/sample - loss: 0.2708 - acc: 0.8926 - val_loss: 0.2692 - val_acc: 0.8975\n",
      "Epoch 57/400\n",
      "17000/21450 [======================>.......] - ETA: 0s - loss: 0.2698 - acc: 0.8952\n",
      "Epoch 00057: val_loss did not improve from 0.26890\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2703 - acc: 0.8946 - val_loss: 0.2760 - val_acc: 0.8945\n",
      "Epoch 58/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2877 - acc: 0.8950\n",
      "Epoch 00058: val_loss improved from 0.26890 to 0.26853, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2693 - acc: 0.8943 - val_loss: 0.2685 - val_acc: 0.8987\n",
      "Epoch 59/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2731 - acc: 0.8870\n",
      "Epoch 00059: val_loss did not improve from 0.26853\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2676 - acc: 0.8951 - val_loss: 0.2692 - val_acc: 0.8990\n",
      "Epoch 60/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2935 - acc: 0.8860\n",
      "Epoch 00060: val_loss did not improve from 0.26853\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2698 - acc: 0.8950 - val_loss: 0.2708 - val_acc: 0.8990\n",
      "Epoch 61/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2830 - acc: 0.8880\n",
      "Epoch 00061: val_loss improved from 0.26853 to 0.26765, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2679 - acc: 0.8948 - val_loss: 0.2676 - val_acc: 0.8978\n",
      "Epoch 62/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2675 - acc: 0.8930\n",
      "Epoch 00062: val_loss did not improve from 0.26765\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2689 - acc: 0.8943 - val_loss: 0.2690 - val_acc: 0.8975\n",
      "Epoch 63/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2888 - acc: 0.8860\n",
      "Epoch 00063: val_loss did not improve from 0.26765\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2679 - acc: 0.8950 - val_loss: 0.2704 - val_acc: 0.8958\n",
      "Epoch 64/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2793 - acc: 0.8890\n",
      "Epoch 00064: val_loss did not improve from 0.26765\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2676 - acc: 0.8955 - val_loss: 0.2716 - val_acc: 0.8965\n",
      "Epoch 65/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2634 - acc: 0.8960\n",
      "Epoch 00065: val_loss did not improve from 0.26765\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2695 - acc: 0.8937 - val_loss: 0.2708 - val_acc: 0.8965\n",
      "Epoch 66/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2661 - acc: 0.8990\n",
      "Epoch 00066: val_loss did not improve from 0.26765\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2707 - acc: 0.8940 - val_loss: 0.2683 - val_acc: 0.8976\n",
      "Epoch 67/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2548 - acc: 0.8950\n",
      "Epoch 00067: val_loss did not improve from 0.26765\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2686 - acc: 0.8941 - val_loss: 0.2761 - val_acc: 0.8934\n",
      "Epoch 68/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2381 - acc: 0.9000\n",
      "Epoch 00068: val_loss did not improve from 0.26765\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2706 - acc: 0.8921 - val_loss: 0.2724 - val_acc: 0.8955\n",
      "Epoch 69/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2686 - acc: 0.8920\n",
      "Epoch 00069: val_loss did not improve from 0.26765\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2675 - acc: 0.8948 - val_loss: 0.2697 - val_acc: 0.8979\n",
      "Epoch 70/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2602 - acc: 0.9010\n",
      "Epoch 00070: val_loss improved from 0.26765 to 0.26751, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2665 - acc: 0.8944 - val_loss: 0.2675 - val_acc: 0.8976\n",
      "Epoch 71/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2863 - acc: 0.8840\n",
      "Epoch 00071: val_loss did not improve from 0.26751\n",
      "21450/21450 [==============================] - 0s 2us/sample - loss: 0.2675 - acc: 0.8932 - val_loss: 0.2731 - val_acc: 0.8968\n",
      "Epoch 72/400\n",
      "19000/21450 [=========================>....] - ETA: 0s - loss: 0.2704 - acc: 0.8927\n",
      "Epoch 00072: val_loss did not improve from 0.26751\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2702 - acc: 0.8936 - val_loss: 0.2686 - val_acc: 0.8989\n",
      "Epoch 73/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2605 - acc: 0.8980\n",
      "Epoch 00073: val_loss did not improve from 0.26751\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2677 - acc: 0.8950 - val_loss: 0.2767 - val_acc: 0.8938\n",
      "Epoch 74/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2540 - acc: 0.9060\n",
      "Epoch 00074: val_loss improved from 0.26751 to 0.26741, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2684 - acc: 0.8943 - val_loss: 0.2674 - val_acc: 0.8976\n",
      "Epoch 75/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2512 - acc: 0.9010\n",
      "Epoch 00075: val_loss did not improve from 0.26741\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2676 - acc: 0.8953 - val_loss: 0.2762 - val_acc: 0.8933\n",
      "Epoch 76/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2634 - acc: 0.9010\n",
      "Epoch 00076: val_loss did not improve from 0.26741\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2687 - acc: 0.8949 - val_loss: 0.2683 - val_acc: 0.8965\n",
      "Epoch 77/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2554 - acc: 0.8900\n",
      "Epoch 00077: val_loss did not improve from 0.26741\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2679 - acc: 0.8941 - val_loss: 0.2696 - val_acc: 0.8959\n",
      "Epoch 78/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2568 - acc: 0.9010\n",
      "Epoch 00078: val_loss did not improve from 0.26741\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2699 - acc: 0.8938 - val_loss: 0.2702 - val_acc: 0.8958\n",
      "Epoch 79/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2853 - acc: 0.8930\n",
      "Epoch 00079: val_loss did not improve from 0.26741\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2677 - acc: 0.8941 - val_loss: 0.2708 - val_acc: 0.8992\n",
      "Epoch 80/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2540 - acc: 0.9020\n",
      "Epoch 00080: val_loss did not improve from 0.26741\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2715 - acc: 0.8929 - val_loss: 0.2675 - val_acc: 0.8983\n",
      "Epoch 81/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2436 - acc: 0.9050\n",
      "Epoch 00081: val_loss did not improve from 0.26741\n",
      "21450/21450 [==============================] - 0s 2us/sample - loss: 0.2680 - acc: 0.8944 - val_loss: 0.2682 - val_acc: 0.8972\n",
      "Epoch 82/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2932 - acc: 0.8800\n",
      "Epoch 00082: val_loss did not improve from 0.26741\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2683 - acc: 0.8944 - val_loss: 0.2684 - val_acc: 0.8971\n",
      "Epoch 83/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2623 - acc: 0.8990\n",
      "Epoch 00083: val_loss did not improve from 0.26741\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2663 - acc: 0.8951 - val_loss: 0.2705 - val_acc: 0.8986\n",
      "Epoch 84/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2672 - acc: 0.8840\n",
      "Epoch 00084: val_loss did not improve from 0.26741\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2722 - acc: 0.8923 - val_loss: 0.2849 - val_acc: 0.8892\n",
      "Epoch 85/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2688 - acc: 0.8960\n",
      "Epoch 00085: val_loss improved from 0.26741 to 0.26286, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2656 - acc: 0.8919 - val_loss: 0.2629 - val_acc: 0.8979\n",
      "Epoch 86/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2478 - acc: 0.8980\n",
      "Epoch 00086: val_loss improved from 0.26286 to 0.26170, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2598 - acc: 0.8957 - val_loss: 0.2617 - val_acc: 0.8976\n",
      "Epoch 87/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2559 - acc: 0.8980\n",
      "Epoch 00087: val_loss improved from 0.26170 to 0.26107, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2589 - acc: 0.8954 - val_loss: 0.2611 - val_acc: 0.8985\n",
      "Epoch 88/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2439 - acc: 0.9080\n",
      "Epoch 00088: val_loss did not improve from 0.26107\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2597 - acc: 0.8949 - val_loss: 0.2646 - val_acc: 0.8969\n",
      "Epoch 89/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.3247 - acc: 0.8630\n",
      "Epoch 00089: val_loss did not improve from 0.26107\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2607 - acc: 0.8946 - val_loss: 0.2615 - val_acc: 0.8973\n",
      "Epoch 90/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2455 - acc: 0.8990\n",
      "Epoch 00090: val_loss did not improve from 0.26107\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2590 - acc: 0.8958 - val_loss: 0.2618 - val_acc: 0.8975\n",
      "Epoch 91/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2428 - acc: 0.8970\n",
      "Epoch 00091: val_loss improved from 0.26107 to 0.26040, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2587 - acc: 0.8954 - val_loss: 0.2604 - val_acc: 0.8982\n",
      "Epoch 92/400\n",
      "14000/21450 [==================>...........] - ETA: 0s - loss: 0.2575 - acc: 0.8963\n",
      "Epoch 00092: val_loss did not improve from 0.26040\n",
      "21450/21450 [==============================] - 0s 4us/sample - loss: 0.2583 - acc: 0.8955 - val_loss: 0.2609 - val_acc: 0.8989\n",
      "Epoch 93/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2626 - acc: 0.8910\n",
      "Epoch 00093: val_loss did not improve from 0.26040\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2599 - acc: 0.8949 - val_loss: 0.2613 - val_acc: 0.8980\n",
      "Epoch 94/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2524 - acc: 0.9010\n",
      "Epoch 00094: val_loss did not improve from 0.26040\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2583 - acc: 0.8956 - val_loss: 0.2609 - val_acc: 0.8986\n",
      "Epoch 95/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2458 - acc: 0.8920\n",
      "Epoch 00095: val_loss did not improve from 0.26040\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2582 - acc: 0.8951 - val_loss: 0.2607 - val_acc: 0.8972\n",
      "Epoch 96/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2564 - acc: 0.8890\n",
      "Epoch 00096: val_loss did not improve from 0.26040\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2579 - acc: 0.8962 - val_loss: 0.2638 - val_acc: 0.8962\n",
      "Epoch 97/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2602 - acc: 0.9030\n",
      "Epoch 00097: val_loss did not improve from 0.26040\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2593 - acc: 0.8945 - val_loss: 0.2614 - val_acc: 0.8962\n",
      "Epoch 98/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2862 - acc: 0.8830\n",
      "Epoch 00098: val_loss improved from 0.26040 to 0.26029, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2585 - acc: 0.8954 - val_loss: 0.2603 - val_acc: 0.8983\n",
      "Epoch 99/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2423 - acc: 0.9030\n",
      "Epoch 00099: val_loss did not improve from 0.26029\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2574 - acc: 0.8963 - val_loss: 0.2609 - val_acc: 0.8982\n",
      "Epoch 100/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2825 - acc: 0.8830\n",
      "Epoch 00100: val_loss did not improve from 0.26029\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2577 - acc: 0.8961 - val_loss: 0.2616 - val_acc: 0.8965\n",
      "Epoch 101/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2573 - acc: 0.8960\n",
      "Epoch 00101: val_loss did not improve from 0.26029\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2596 - acc: 0.8944 - val_loss: 0.2609 - val_acc: 0.8992\n",
      "Epoch 102/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2376 - acc: 0.9130\n",
      "Epoch 00102: val_loss did not improve from 0.26029\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2591 - acc: 0.8956 - val_loss: 0.2655 - val_acc: 0.8962\n",
      "Epoch 103/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2654 - acc: 0.8850\n",
      "Epoch 00103: val_loss improved from 0.26029 to 0.26022, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2599 - acc: 0.8943 - val_loss: 0.2602 - val_acc: 0.8993\n",
      "Epoch 104/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2788 - acc: 0.9010\n",
      "Epoch 00104: val_loss did not improve from 0.26022\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2577 - acc: 0.8963 - val_loss: 0.2604 - val_acc: 0.8975\n",
      "Epoch 105/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2648 - acc: 0.9000\n",
      "Epoch 00105: val_loss did not improve from 0.26022\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2574 - acc: 0.8963 - val_loss: 0.2605 - val_acc: 0.8979\n",
      "Epoch 106/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2234 - acc: 0.9100\n",
      "Epoch 00106: val_loss did not improve from 0.26022\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2571 - acc: 0.8963 - val_loss: 0.2612 - val_acc: 0.8968\n",
      "Epoch 107/400\n",
      "21000/21450 [============================>.] - ETA: 0s - loss: 0.2572 - acc: 0.8963\n",
      "Epoch 00107: val_loss did not improve from 0.26022\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2579 - acc: 0.8958 - val_loss: 0.2628 - val_acc: 0.8965\n",
      "Epoch 108/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2484 - acc: 0.9010\n",
      "Epoch 00108: val_loss did not improve from 0.26022\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2582 - acc: 0.8953 - val_loss: 0.2608 - val_acc: 0.8978\n",
      "Epoch 109/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2656 - acc: 0.8880\n",
      "Epoch 00109: val_loss improved from 0.26022 to 0.25817, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2543 - acc: 0.8963 - val_loss: 0.2582 - val_acc: 0.8985\n",
      "Epoch 110/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2431 - acc: 0.9110\n",
      "Epoch 00110: val_loss improved from 0.25817 to 0.25739, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2536 - acc: 0.8959 - val_loss: 0.2574 - val_acc: 0.8980\n",
      "Epoch 111/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2532 - acc: 0.8880\n",
      "Epoch 00111: val_loss did not improve from 0.25739\n",
      "21450/21450 [==============================] - 0s 2us/sample - loss: 0.2532 - acc: 0.8958 - val_loss: 0.2583 - val_acc: 0.8971\n",
      "Epoch 112/400\n",
      "18000/21450 [========================>.....] - ETA: 0s - loss: 0.2537 - acc: 0.8963\n",
      "Epoch 00112: val_loss did not improve from 0.25739\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2537 - acc: 0.8959 - val_loss: 0.2575 - val_acc: 0.8976\n",
      "Epoch 113/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2418 - acc: 0.8940\n",
      "Epoch 00113: val_loss did not improve from 0.25739\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2530 - acc: 0.8965 - val_loss: 0.2580 - val_acc: 0.8975\n",
      "Epoch 114/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2724 - acc: 0.8870\n",
      "Epoch 00114: val_loss did not improve from 0.25739\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2537 - acc: 0.8964 - val_loss: 0.2589 - val_acc: 0.8976\n",
      "Epoch 115/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2521 - acc: 0.8970\n",
      "Epoch 00115: val_loss did not improve from 0.25739\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2539 - acc: 0.8959 - val_loss: 0.2575 - val_acc: 0.8985\n",
      "Epoch 116/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2446 - acc: 0.9020\n",
      "Epoch 00116: val_loss did not improve from 0.25739\n",
      "21450/21450 [==============================] - 0s 2us/sample - loss: 0.2537 - acc: 0.8963 - val_loss: 0.2585 - val_acc: 0.8982\n",
      "Epoch 117/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2418 - acc: 0.8970\n",
      "Epoch 00117: val_loss did not improve from 0.25739\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2540 - acc: 0.8952 - val_loss: 0.2575 - val_acc: 0.8985\n",
      "Epoch 118/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2615 - acc: 0.8860\n",
      "Epoch 00118: val_loss did not improve from 0.25739\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2538 - acc: 0.8963 - val_loss: 0.2575 - val_acc: 0.8987\n",
      "Epoch 119/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2474 - acc: 0.8850\n",
      "Epoch 00119: val_loss did not improve from 0.25739\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2534 - acc: 0.8959 - val_loss: 0.2580 - val_acc: 0.8980\n",
      "Epoch 120/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2466 - acc: 0.9000\n",
      "Epoch 00120: val_loss did not improve from 0.25739\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2533 - acc: 0.8962 - val_loss: 0.2579 - val_acc: 0.8969\n",
      "Epoch 121/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2860 - acc: 0.8770\n",
      "Epoch 00121: val_loss improved from 0.25739 to 0.25581, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2519 - acc: 0.8961 - val_loss: 0.2558 - val_acc: 0.8980\n",
      "Epoch 122/400\n",
      "21000/21450 [============================>.] - ETA: 0s - loss: 0.2509 - acc: 0.8967\n",
      "Epoch 00122: val_loss did not improve from 0.25581\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2510 - acc: 0.8965 - val_loss: 0.2562 - val_acc: 0.8973\n",
      "Epoch 123/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2344 - acc: 0.9000\n",
      "Epoch 00123: val_loss did not improve from 0.25581\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2510 - acc: 0.8967 - val_loss: 0.2561 - val_acc: 0.8973\n",
      "Epoch 124/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2479 - acc: 0.8940\n",
      "Epoch 00124: val_loss did not improve from 0.25581\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2509 - acc: 0.8963 - val_loss: 0.2559 - val_acc: 0.8976\n",
      "Epoch 125/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2475 - acc: 0.9030\n",
      "Epoch 00125: val_loss did not improve from 0.25581\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2510 - acc: 0.8967 - val_loss: 0.2561 - val_acc: 0.8980\n",
      "Epoch 126/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2434 - acc: 0.8950\n",
      "Epoch 00126: val_loss did not improve from 0.25581\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2507 - acc: 0.8967 - val_loss: 0.2559 - val_acc: 0.8980\n",
      "Epoch 127/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2570 - acc: 0.8990\n",
      "Epoch 00127: val_loss improved from 0.25581 to 0.25581, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2507 - acc: 0.8966 - val_loss: 0.2558 - val_acc: 0.8979\n",
      "Epoch 128/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2665 - acc: 0.8830\n",
      "Epoch 00128: val_loss improved from 0.25581 to 0.25547, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2506 - acc: 0.8969 - val_loss: 0.2555 - val_acc: 0.8987\n",
      "Epoch 129/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2444 - acc: 0.9020\n",
      "Epoch 00129: val_loss did not improve from 0.25547\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2510 - acc: 0.8966 - val_loss: 0.2556 - val_acc: 0.8985\n",
      "Epoch 130/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2629 - acc: 0.8980\n",
      "Epoch 00130: val_loss did not improve from 0.25547\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2511 - acc: 0.8960 - val_loss: 0.2562 - val_acc: 0.8980\n",
      "Epoch 131/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2224 - acc: 0.9020\n",
      "Epoch 00131: val_loss did not improve from 0.25547\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2512 - acc: 0.8969 - val_loss: 0.2562 - val_acc: 0.8982\n",
      "Epoch 132/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2395 - acc: 0.9050\n",
      "Epoch 00132: val_loss did not improve from 0.25547\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2509 - acc: 0.8966 - val_loss: 0.2558 - val_acc: 0.8976\n",
      "Epoch 133/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2363 - acc: 0.9140\n",
      "Epoch 00133: val_loss did not improve from 0.25547\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2509 - acc: 0.8960 - val_loss: 0.2556 - val_acc: 0.8975\n",
      "Epoch 134/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2548 - acc: 0.8960\n",
      "Epoch 00134: val_loss did not improve from 0.25547\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2506 - acc: 0.8964 - val_loss: 0.2561 - val_acc: 0.8975\n",
      "Epoch 135/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2662 - acc: 0.8880\n",
      "Epoch 00135: val_loss improved from 0.25547 to 0.25541, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2507 - acc: 0.8965 - val_loss: 0.2554 - val_acc: 0.8973\n",
      "Epoch 136/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2694 - acc: 0.8810\n",
      "Epoch 00136: val_loss did not improve from 0.25541\n",
      "21450/21450 [==============================] - 0s 2us/sample - loss: 0.2505 - acc: 0.8972 - val_loss: 0.2557 - val_acc: 0.8975\n",
      "Epoch 137/400\n",
      "21000/21450 [============================>.] - ETA: 0s - loss: 0.2503 - acc: 0.8968\n",
      "Epoch 00137: val_loss did not improve from 0.25541\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2505 - acc: 0.8966 - val_loss: 0.2557 - val_acc: 0.8972\n",
      "Epoch 138/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2569 - acc: 0.8940\n",
      "Epoch 00138: val_loss did not improve from 0.25541\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2506 - acc: 0.8969 - val_loss: 0.2558 - val_acc: 0.8978\n",
      "Epoch 139/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2819 - acc: 0.8830\n",
      "Epoch 00139: val_loss improved from 0.25541 to 0.25492, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2500 - acc: 0.8967 - val_loss: 0.2549 - val_acc: 0.8972\n",
      "Epoch 140/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2570 - acc: 0.8990\n",
      "Epoch 00140: val_loss improved from 0.25492 to 0.25472, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2494 - acc: 0.8965 - val_loss: 0.2547 - val_acc: 0.8982\n",
      "Epoch 141/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2397 - acc: 0.8930\n",
      "Epoch 00141: val_loss did not improve from 0.25472\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2495 - acc: 0.8961 - val_loss: 0.2551 - val_acc: 0.8976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2485 - acc: 0.8930\n",
      "Epoch 00142: val_loss did not improve from 0.25472\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2495 - acc: 0.8966 - val_loss: 0.2549 - val_acc: 0.8976\n",
      "Epoch 143/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2186 - acc: 0.9180\n",
      "Epoch 00143: val_loss improved from 0.25472 to 0.25470, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2497 - acc: 0.8966 - val_loss: 0.2547 - val_acc: 0.8979\n",
      "Epoch 144/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2743 - acc: 0.8880\n",
      "Epoch 00144: val_loss did not improve from 0.25470\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2497 - acc: 0.8967 - val_loss: 0.2552 - val_acc: 0.8978\n",
      "Epoch 145/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2546 - acc: 0.8970\n",
      "Epoch 00145: val_loss improved from 0.25470 to 0.25452, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2494 - acc: 0.8966 - val_loss: 0.2545 - val_acc: 0.8978\n",
      "Epoch 146/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2596 - acc: 0.8920\n",
      "Epoch 00146: val_loss did not improve from 0.25452\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2493 - acc: 0.8964 - val_loss: 0.2546 - val_acc: 0.8978\n",
      "Epoch 147/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2590 - acc: 0.8910\n",
      "Epoch 00147: val_loss did not improve from 0.25452\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2493 - acc: 0.8967 - val_loss: 0.2547 - val_acc: 0.8972\n",
      "Epoch 148/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2726 - acc: 0.8810\n",
      "Epoch 00148: val_loss did not improve from 0.25452\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2494 - acc: 0.8969 - val_loss: 0.2548 - val_acc: 0.8979\n",
      "Epoch 149/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2785 - acc: 0.8850\n",
      "Epoch 00149: val_loss did not improve from 0.25452\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2494 - acc: 0.8964 - val_loss: 0.2547 - val_acc: 0.8978\n",
      "Epoch 150/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2517 - acc: 0.8900\n",
      "Epoch 00150: val_loss did not improve from 0.25452\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2498 - acc: 0.8968 - val_loss: 0.2547 - val_acc: 0.8975\n",
      "Epoch 151/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2700 - acc: 0.8870\n",
      "Epoch 00151: val_loss did not improve from 0.25452\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2494 - acc: 0.8966 - val_loss: 0.2550 - val_acc: 0.8976\n",
      "Epoch 152/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2406 - acc: 0.8910\n",
      "Epoch 00152: val_loss did not improve from 0.25452\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2494 - acc: 0.8963 - val_loss: 0.2550 - val_acc: 0.8979\n",
      "Epoch 153/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2510 - acc: 0.8970\n",
      "Epoch 00153: val_loss did not improve from 0.25452\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2494 - acc: 0.8968 - val_loss: 0.2548 - val_acc: 0.8975\n",
      "Epoch 154/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2550 - acc: 0.8900\n",
      "Epoch 00154: val_loss did not improve from 0.25452\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2494 - acc: 0.8972 - val_loss: 0.2547 - val_acc: 0.8975\n",
      "Epoch 155/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2437 - acc: 0.8990\n",
      "Epoch 00155: val_loss did not improve from 0.25452\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2492 - acc: 0.8969 - val_loss: 0.2549 - val_acc: 0.8978\n",
      "Epoch 156/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2678 - acc: 0.8870\n",
      "Epoch 00156: val_loss improved from 0.25452 to 0.25448, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2488 - acc: 0.8968 - val_loss: 0.2545 - val_acc: 0.8973\n",
      "Epoch 157/400\n",
      "21000/21450 [============================>.] - ETA: 0s - loss: 0.2488 - acc: 0.8965\n",
      "Epoch 00157: val_loss improved from 0.25448 to 0.25414, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2487 - acc: 0.8965 - val_loss: 0.2541 - val_acc: 0.8975\n",
      "Epoch 158/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2765 - acc: 0.8760\n",
      "Epoch 00158: val_loss did not improve from 0.25414\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2486 - acc: 0.8967 - val_loss: 0.2542 - val_acc: 0.8975\n",
      "Epoch 159/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2192 - acc: 0.9090\n",
      "Epoch 00159: val_loss did not improve from 0.25414\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2486 - acc: 0.8963 - val_loss: 0.2542 - val_acc: 0.8976\n",
      "Epoch 160/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2331 - acc: 0.9050\n",
      "Epoch 00160: val_loss did not improve from 0.25414\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2486 - acc: 0.8969 - val_loss: 0.2543 - val_acc: 0.8973\n",
      "Epoch 161/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2569 - acc: 0.8960\n",
      "Epoch 00161: val_loss did not improve from 0.25414\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2486 - acc: 0.8970 - val_loss: 0.2542 - val_acc: 0.8973\n",
      "Epoch 162/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2299 - acc: 0.8960\n",
      "Epoch 00162: val_loss improved from 0.25414 to 0.25413, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2486 - acc: 0.8966 - val_loss: 0.2541 - val_acc: 0.8978\n",
      "Epoch 163/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2185 - acc: 0.9080\n",
      "Epoch 00163: val_loss improved from 0.25413 to 0.25407, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2487 - acc: 0.8960 - val_loss: 0.2541 - val_acc: 0.8976\n",
      "Epoch 164/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2506 - acc: 0.8840\n",
      "Epoch 00164: val_loss improved from 0.25407 to 0.25404, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2486 - acc: 0.8968 - val_loss: 0.2540 - val_acc: 0.8972\n",
      "Epoch 165/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2454 - acc: 0.8950\n",
      "Epoch 00165: val_loss did not improve from 0.25404\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2486 - acc: 0.8967 - val_loss: 0.2541 - val_acc: 0.8973\n",
      "Epoch 166/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2706 - acc: 0.8990\n",
      "Epoch 00166: val_loss did not improve from 0.25404\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2485 - acc: 0.8965 - val_loss: 0.2541 - val_acc: 0.8983\n",
      "Epoch 167/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2391 - acc: 0.8960\n",
      "Epoch 00167: val_loss did not improve from 0.25404\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2487 - acc: 0.8966 - val_loss: 0.2542 - val_acc: 0.8978\n",
      "Epoch 168/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2355 - acc: 0.8970\n",
      "Epoch 00168: val_loss improved from 0.25404 to 0.25389, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2483 - acc: 0.8966 - val_loss: 0.2539 - val_acc: 0.8976\n",
      "Epoch 169/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2457 - acc: 0.9020\n",
      "Epoch 00169: val_loss improved from 0.25389 to 0.25384, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2483 - acc: 0.8967 - val_loss: 0.2538 - val_acc: 0.8973\n",
      "Epoch 170/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2346 - acc: 0.9040\n",
      "Epoch 00170: val_loss improved from 0.25384 to 0.25384, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2482 - acc: 0.8966 - val_loss: 0.2538 - val_acc: 0.8975\n",
      "Epoch 171/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2302 - acc: 0.9090\n",
      "Epoch 00171: val_loss did not improve from 0.25384\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2482 - acc: 0.8965 - val_loss: 0.2538 - val_acc: 0.8975\n",
      "Epoch 172/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2566 - acc: 0.9000\n",
      "Epoch 00172: val_loss did not improve from 0.25384\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2483 - acc: 0.8967 - val_loss: 0.2539 - val_acc: 0.8975\n",
      "Epoch 173/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2617 - acc: 0.8820\n",
      "Epoch 00173: val_loss improved from 0.25384 to 0.25381, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2482 - acc: 0.8966 - val_loss: 0.2538 - val_acc: 0.8976\n",
      "Epoch 174/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2674 - acc: 0.8850\n",
      "Epoch 00174: val_loss did not improve from 0.25381\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2482 - acc: 0.8969 - val_loss: 0.2539 - val_acc: 0.8975\n",
      "Epoch 175/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2491 - acc: 0.8950\n",
      "Epoch 00175: val_loss improved from 0.25381 to 0.25379, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2482 - acc: 0.8968 - val_loss: 0.2538 - val_acc: 0.8975\n",
      "Epoch 176/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2325 - acc: 0.9170\n",
      "Epoch 00176: val_loss did not improve from 0.25379\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2482 - acc: 0.8967 - val_loss: 0.2539 - val_acc: 0.8975\n",
      "Epoch 177/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2602 - acc: 0.8960\n",
      "Epoch 00177: val_loss did not improve from 0.25379\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2482 - acc: 0.8967 - val_loss: 0.2539 - val_acc: 0.8978\n",
      "Epoch 178/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2721 - acc: 0.8800\n",
      "Epoch 00178: val_loss did not improve from 0.25379\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2482 - acc: 0.8967 - val_loss: 0.2538 - val_acc: 0.8976\n",
      "Epoch 179/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2376 - acc: 0.9060\n",
      "Epoch 00179: val_loss improved from 0.25379 to 0.25374, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2481 - acc: 0.8969 - val_loss: 0.2537 - val_acc: 0.8976\n",
      "Epoch 180/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2404 - acc: 0.8940\n",
      "Epoch 00180: val_loss improved from 0.25374 to 0.25371, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2481 - acc: 0.8966 - val_loss: 0.2537 - val_acc: 0.8975\n",
      "Epoch 181/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2285 - acc: 0.9130\n",
      "Epoch 00181: val_loss did not improve from 0.25371\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2480 - acc: 0.8969 - val_loss: 0.2537 - val_acc: 0.8975\n",
      "Epoch 182/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2504 - acc: 0.9010\n",
      "Epoch 00182: val_loss improved from 0.25371 to 0.25371, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2481 - acc: 0.8967 - val_loss: 0.2537 - val_acc: 0.8976\n",
      "Epoch 183/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2367 - acc: 0.9040\n",
      "Epoch 00183: val_loss did not improve from 0.25371\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2481 - acc: 0.8968 - val_loss: 0.2537 - val_acc: 0.8978\n",
      "Epoch 184/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2501 - acc: 0.8970\n",
      "Epoch 00184: val_loss improved from 0.25371 to 0.25371, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2480 - acc: 0.8967 - val_loss: 0.2537 - val_acc: 0.8976\n",
      "Epoch 185/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2353 - acc: 0.9070\n",
      "Epoch 00185: val_loss did not improve from 0.25371\n",
      "21450/21450 [==============================] - 0s 2us/sample - loss: 0.2480 - acc: 0.8969 - val_loss: 0.2537 - val_acc: 0.8976\n",
      "Epoch 186/400\n",
      "20000/21450 [==========================>...] - ETA: 0s - loss: 0.2472 - acc: 0.8971\n",
      "Epoch 00186: val_loss did not improve from 0.25371\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2480 - acc: 0.8966 - val_loss: 0.2537 - val_acc: 0.8973\n",
      "Epoch 187/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2518 - acc: 0.8850\n",
      "Epoch 00187: val_loss did not improve from 0.25371\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2480 - acc: 0.8968 - val_loss: 0.2537 - val_acc: 0.8973\n",
      "Epoch 188/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2527 - acc: 0.8960\n",
      "Epoch 00188: val_loss improved from 0.25371 to 0.25369, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2480 - acc: 0.8968 - val_loss: 0.2537 - val_acc: 0.8978\n",
      "Epoch 189/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2448 - acc: 0.9050\n",
      "Epoch 00189: val_loss did not improve from 0.25369\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2480 - acc: 0.8966 - val_loss: 0.2537 - val_acc: 0.8976\n",
      "Epoch 190/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2684 - acc: 0.8820\n",
      "Epoch 00190: val_loss improved from 0.25369 to 0.25368, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2480 - acc: 0.8969 - val_loss: 0.2537 - val_acc: 0.8976\n",
      "Epoch 191/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2452 - acc: 0.8960\n",
      "Epoch 00191: val_loss improved from 0.25368 to 0.25366, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2480 - acc: 0.8969 - val_loss: 0.2537 - val_acc: 0.8973\n",
      "Epoch 192/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2334 - acc: 0.8980\n",
      "Epoch 00192: val_loss improved from 0.25366 to 0.25364, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8969 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 193/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2471 - acc: 0.9150\n",
      "Epoch 00193: val_loss improved from 0.25364 to 0.25364, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8969 - val_loss: 0.2536 - val_acc: 0.8973\n",
      "Epoch 194/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2479 - acc: 0.8930\n",
      "Epoch 00194: val_loss did not improve from 0.25364\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2537 - val_acc: 0.8976\n",
      "Epoch 195/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2579 - acc: 0.8810\n",
      "Epoch 00195: val_loss did not improve from 0.25364\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2480 - acc: 0.8968 - val_loss: 0.2537 - val_acc: 0.8976\n",
      "Epoch 196/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2639 - acc: 0.8890\n",
      "Epoch 00196: val_loss did not improve from 0.25364\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8975\n",
      "Epoch 197/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2538 - acc: 0.8950\n",
      "Epoch 00197: val_loss did not improve from 0.25364\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2480 - acc: 0.8969 - val_loss: 0.2537 - val_acc: 0.8978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2562 - acc: 0.8920\n",
      "Epoch 00198: val_loss did not improve from 0.25364\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8969 - val_loss: 0.2537 - val_acc: 0.8976\n",
      "Epoch 199/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2323 - acc: 0.9020\n",
      "Epoch 00199: val_loss did not improve from 0.25364\n",
      "21450/21450 [==============================] - 0s 2us/sample - loss: 0.2479 - acc: 0.8969 - val_loss: 0.2536 - val_acc: 0.8975\n",
      "Epoch 200/400\n",
      "20000/21450 [==========================>...] - ETA: 0s - loss: 0.2473 - acc: 0.8969\n",
      "Epoch 00200: val_loss improved from 0.25364 to 0.25362, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 4us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8975\n",
      "Epoch 201/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2232 - acc: 0.9030\n",
      "Epoch 00201: val_loss improved from 0.25362 to 0.25362, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 202/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2510 - acc: 0.8940\n",
      "Epoch 00202: val_loss improved from 0.25362 to 0.25361, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 203/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2509 - acc: 0.8900\n",
      "Epoch 00203: val_loss did not improve from 0.25361\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 204/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2166 - acc: 0.9210\n",
      "Epoch 00204: val_loss improved from 0.25361 to 0.25361, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 205/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2401 - acc: 0.9060\n",
      "Epoch 00205: val_loss improved from 0.25361 to 0.25361, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 206/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2575 - acc: 0.8970\n",
      "Epoch 00206: val_loss improved from 0.25361 to 0.25360, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 207/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2282 - acc: 0.9010\n",
      "Epoch 00207: val_loss did not improve from 0.25360\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 208/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2596 - acc: 0.8870\n",
      "Epoch 00208: val_loss improved from 0.25360 to 0.25360, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8976\n",
      "Epoch 209/400\n",
      "21000/21450 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.8968\n",
      "Epoch 00209: val_loss did not improve from 0.25360\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8969 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 210/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2483 - acc: 0.8980\n",
      "Epoch 00210: val_loss did not improve from 0.25360\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8976\n",
      "Epoch 211/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2466 - acc: 0.9030\n",
      "Epoch 00211: val_loss improved from 0.25360 to 0.25360, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8976\n",
      "Epoch 212/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2182 - acc: 0.9110\n",
      "Epoch 00212: val_loss improved from 0.25360 to 0.25359, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 213/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2705 - acc: 0.8870\n",
      "Epoch 00213: val_loss did not improve from 0.25359\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 214/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2296 - acc: 0.9000\n",
      "Epoch 00214: val_loss did not improve from 0.25359\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8969 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 215/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2384 - acc: 0.9020\n",
      "Epoch 00215: val_loss did not improve from 0.25359\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 216/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2510 - acc: 0.8990\n",
      "Epoch 00216: val_loss did not improve from 0.25359\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 217/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2536 - acc: 0.8920\n",
      "Epoch 00217: val_loss did not improve from 0.25359\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 218/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2416 - acc: 0.8960\n",
      "Epoch 00218: val_loss did not improve from 0.25359\n",
      "21450/21450 [==============================] - 0s 2us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 219/400\n",
      "21000/21450 [============================>.] - ETA: 0s - loss: 0.2482 - acc: 0.8965\n",
      "Epoch 00219: val_loss did not improve from 0.25359\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 220/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2668 - acc: 0.8920\n",
      "Epoch 00220: val_loss did not improve from 0.25359\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8975\n",
      "Epoch 221/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2393 - acc: 0.9040\n",
      "Epoch 00221: val_loss improved from 0.25359 to 0.25359, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 222/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2640 - acc: 0.8880\n",
      "Epoch 00222: val_loss improved from 0.25359 to 0.25359, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 223/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2468 - acc: 0.8870\n",
      "Epoch 00223: val_loss improved from 0.25359 to 0.25359, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 224/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.1917 - acc: 0.9270\n",
      "Epoch 00224: val_loss improved from 0.25359 to 0.25359, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 225/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2435 - acc: 0.9120\n",
      "Epoch 00225: val_loss did not improve from 0.25359\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 226/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2489 - acc: 0.8900\n",
      "Epoch 00226: val_loss did not improve from 0.25359\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 227/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2569 - acc: 0.8890\n",
      "Epoch 00227: val_loss improved from 0.25359 to 0.25359, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 228/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2461 - acc: 0.8930\n",
      "Epoch 00228: val_loss improved from 0.25359 to 0.25359, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 229/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2300 - acc: 0.9010\n",
      "Epoch 00229: val_loss did not improve from 0.25359\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 230/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2415 - acc: 0.8940\n",
      "Epoch 00230: val_loss did not improve from 0.25359\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 231/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2663 - acc: 0.8920\n",
      "Epoch 00231: val_loss improved from 0.25359 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 232/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2583 - acc: 0.8930\n",
      "Epoch 00232: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 233/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2412 - acc: 0.9020\n",
      "Epoch 00233: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 234/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2530 - acc: 0.8880\n",
      "Epoch 00234: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 235/400\n",
      "18000/21450 [========================>.....] - ETA: 0s - loss: 0.2447 - acc: 0.8994\n",
      "Epoch 00235: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 236/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2628 - acc: 0.8910\n",
      "Epoch 00236: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 237/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2853 - acc: 0.8780\n",
      "Epoch 00237: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 2us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 238/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2475 - acc: 0.9000\n",
      "Epoch 00238: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 239/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2497 - acc: 0.8980\n",
      "Epoch 00239: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 240/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2439 - acc: 0.8950\n",
      "Epoch 00240: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 241/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2469 - acc: 0.8900\n",
      "Epoch 00241: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 4us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 242/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2421 - acc: 0.8970\n",
      "Epoch 00242: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 243/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2589 - acc: 0.8910\n",
      "Epoch 00243: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 244/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2435 - acc: 0.9060\n",
      "Epoch 00244: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 245/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2421 - acc: 0.8880\n",
      "Epoch 00245: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 246/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2360 - acc: 0.9080\n",
      "Epoch 00246: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 247/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2702 - acc: 0.8810\n",
      "Epoch 00247: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 248/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2725 - acc: 0.8720\n",
      "Epoch 00248: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 249/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2677 - acc: 0.8870\n",
      "Epoch 00249: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 250/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2636 - acc: 0.8950\n",
      "Epoch 00250: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 251/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2465 - acc: 0.8960\n",
      "Epoch 00251: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 252/400\n",
      "19000/21450 [=========================>....] - ETA: 0s - loss: 0.2488 - acc: 0.8959\n",
      "Epoch 00252: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 253/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2324 - acc: 0.9000\n",
      "Epoch 00253: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 254/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2443 - acc: 0.8950\n",
      "Epoch 00254: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 255/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2591 - acc: 0.8900\n",
      "Epoch 00255: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 256/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2340 - acc: 0.8930\n",
      "Epoch 00256: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 257/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2467 - acc: 0.8890\n",
      "Epoch 00257: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 258/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2450 - acc: 0.9050\n",
      "Epoch 00258: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 259/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2387 - acc: 0.8990\n",
      "Epoch 00259: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 260/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2365 - acc: 0.9050\n",
      "Epoch 00260: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2479 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 261/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2346 - acc: 0.8930\n",
      "Epoch 00261: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 262/400\n",
      "19000/21450 [=========================>....] - ETA: 0s - loss: 0.2464 - acc: 0.8973\n",
      "Epoch 00262: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 263/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2261 - acc: 0.9110\n",
      "Epoch 00263: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 264/400\n",
      "20000/21450 [==========================>...] - ETA: 0s - loss: 0.2470 - acc: 0.8971\n",
      "Epoch 00264: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 265/400\n",
      "21000/21450 [============================>.] - ETA: 0s - loss: 0.2477 - acc: 0.8968\n",
      "Epoch 00265: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 266/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2384 - acc: 0.9060\n",
      "Epoch 00266: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 267/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2588 - acc: 0.8950\n",
      "Epoch 00267: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 268/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2707 - acc: 0.8860\n",
      "Epoch 00268: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 269/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2468 - acc: 0.9100\n",
      "Epoch 00269: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 270/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2389 - acc: 0.8930\n",
      "Epoch 00270: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 271/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2118 - acc: 0.9130\n",
      "Epoch 00271: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 272/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2440 - acc: 0.8920\n",
      "Epoch 00272: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 273/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2740 - acc: 0.8820\n",
      "Epoch 00273: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 274/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2483 - acc: 0.8960\n",
      "Epoch 00274: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 275/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2334 - acc: 0.9120\n",
      "Epoch 00275: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 2us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 276/400\n",
      "20000/21450 [==========================>...] - ETA: 0s - loss: 0.2475 - acc: 0.8969\n",
      "Epoch 00276: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 277/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2469 - acc: 0.9050\n",
      "Epoch 00277: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8967 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 278/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2280 - acc: 0.9040\n",
      "Epoch 00278: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 279/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2874 - acc: 0.8740\n",
      "Epoch 00279: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 280/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2562 - acc: 0.8940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00280: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 281/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2340 - acc: 0.8970\n",
      "Epoch 00281: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 4us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 282/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2494 - acc: 0.8960\n",
      "Epoch 00282: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 283/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2158 - acc: 0.9030\n",
      "Epoch 00283: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 284/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2620 - acc: 0.8840\n",
      "Epoch 00284: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 285/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2242 - acc: 0.9010\n",
      "Epoch 00285: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 286/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2543 - acc: 0.8990\n",
      "Epoch 00286: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 287/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2300 - acc: 0.9000\n",
      "Epoch 00287: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 288/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2054 - acc: 0.9140\n",
      "Epoch 00288: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 289/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2600 - acc: 0.8900\n",
      "Epoch 00289: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 290/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2642 - acc: 0.8830\n",
      "Epoch 00290: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 291/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2482 - acc: 0.8950\n",
      "Epoch 00291: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 292/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2474 - acc: 0.9120\n",
      "Epoch 00292: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 293/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2597 - acc: 0.8950\n",
      "Epoch 00293: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 294/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2801 - acc: 0.8890\n",
      "Epoch 00294: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 2us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 295/400\n",
      "20000/21450 [==========================>...] - ETA: 0s - loss: 0.2471 - acc: 0.8964\n",
      "Epoch 00295: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 296/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2228 - acc: 0.9070\n",
      "Epoch 00296: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 297/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2322 - acc: 0.9000\n",
      "Epoch 00297: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 298/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2377 - acc: 0.8970\n",
      "Epoch 00298: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 299/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2429 - acc: 0.8970\n",
      "Epoch 00299: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 300/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2433 - acc: 0.8990\n",
      "Epoch 00300: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 301/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2423 - acc: 0.9040\n",
      "Epoch 00301: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 302/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2732 - acc: 0.8920\n",
      "Epoch 00302: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 303/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2504 - acc: 0.9000\n",
      "Epoch 00303: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 304/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2757 - acc: 0.8820\n",
      "Epoch 00304: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 305/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2440 - acc: 0.8960\n",
      "Epoch 00305: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 306/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2582 - acc: 0.8910\n",
      "Epoch 00306: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 307/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2435 - acc: 0.9100\n",
      "Epoch 00307: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 308/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2557 - acc: 0.8900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00308: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 309/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2464 - acc: 0.9060\n",
      "Epoch 00309: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 2us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 310/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2677 - acc: 0.8830\n",
      "Epoch 00310: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 311/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2498 - acc: 0.9050\n",
      "Epoch 00311: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 312/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2446 - acc: 0.8930\n",
      "Epoch 00312: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 313/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2605 - acc: 0.8920\n",
      "Epoch 00313: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 314/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2317 - acc: 0.9020\n",
      "Epoch 00314: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 315/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2691 - acc: 0.8910\n",
      "Epoch 00315: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 316/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2493 - acc: 0.8870\n",
      "Epoch 00316: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 317/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2539 - acc: 0.8950\n",
      "Epoch 00317: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 318/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2437 - acc: 0.8960\n",
      "Epoch 00318: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 319/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2438 - acc: 0.8950\n",
      "Epoch 00319: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 320/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2671 - acc: 0.8920\n",
      "Epoch 00320: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 321/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2721 - acc: 0.8820\n",
      "Epoch 00321: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 322/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2505 - acc: 0.9080\n",
      "Epoch 00322: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 323/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2565 - acc: 0.8920\n",
      "Epoch 00323: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 324/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2487 - acc: 0.9040\n",
      "Epoch 00324: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 325/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2348 - acc: 0.9050\n",
      "Epoch 00325: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 326/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2303 - acc: 0.9030\n",
      "Epoch 00326: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 327/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2675 - acc: 0.8860\n",
      "Epoch 00327: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 328/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2770 - acc: 0.8740\n",
      "Epoch 00328: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 329/400\n",
      "21000/21450 [============================>.] - ETA: 0s - loss: 0.2478 - acc: 0.8970\n",
      "Epoch 00329: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 330/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2482 - acc: 0.8990\n",
      "Epoch 00330: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 331/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2470 - acc: 0.8900\n",
      "Epoch 00331: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 332/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2299 - acc: 0.9010\n",
      "Epoch 00332: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 333/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2381 - acc: 0.9100\n",
      "Epoch 00333: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 334/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2321 - acc: 0.8980\n",
      "Epoch 00334: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 335/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2247 - acc: 0.9130\n",
      "Epoch 00335: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 336/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2403 - acc: 0.9050\n",
      "Epoch 00336: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 337/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2447 - acc: 0.8980\n",
      "Epoch 00337: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 338/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2479 - acc: 0.9020\n",
      "Epoch 00338: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 2us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 339/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2477 - acc: 0.8920\n",
      "Epoch 00339: val_loss improved from 0.25358 to 0.25358, saving model to models_chpt/best_modelD.hdf5\n",
      "21450/21450 [==============================] - 0s 4us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 340/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2316 - acc: 0.9070\n",
      "Epoch 00340: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 341/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2554 - acc: 0.8990\n",
      "Epoch 00341: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 342/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2677 - acc: 0.8820\n",
      "Epoch 00342: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 343/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2575 - acc: 0.8920\n",
      "Epoch 00343: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 344/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2464 - acc: 0.8980\n",
      "Epoch 00344: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 345/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2563 - acc: 0.8950\n",
      "Epoch 00345: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 346/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2775 - acc: 0.8740\n",
      "Epoch 00346: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 347/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2422 - acc: 0.8910\n",
      "Epoch 00347: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 348/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2434 - acc: 0.8980\n",
      "Epoch 00348: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 349/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2577 - acc: 0.8950\n",
      "Epoch 00349: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 350/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2390 - acc: 0.9050\n",
      "Epoch 00350: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 351/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2360 - acc: 0.9090\n",
      "Epoch 00351: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 352/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2444 - acc: 0.8930\n",
      "Epoch 00352: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 353/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2401 - acc: 0.8910\n",
      "Epoch 00353: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 354/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2298 - acc: 0.9080\n",
      "Epoch 00354: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 355/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2323 - acc: 0.8920\n",
      "Epoch 00355: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 356/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2765 - acc: 0.8870\n",
      "Epoch 00356: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 357/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2540 - acc: 0.8980\n",
      "Epoch 00357: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 358/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2524 - acc: 0.8970\n",
      "Epoch 00358: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 359/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2618 - acc: 0.8880\n",
      "Epoch 00359: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 360/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2590 - acc: 0.8910\n",
      "Epoch 00360: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 361/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2439 - acc: 0.8960\n",
      "Epoch 00361: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 362/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2424 - acc: 0.9020\n",
      "Epoch 00362: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 363/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2431 - acc: 0.8970\n",
      "Epoch 00363: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 364/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2414 - acc: 0.9110\n",
      "Epoch 00364: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 365/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2355 - acc: 0.9050\n",
      "Epoch 00365: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 366/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2437 - acc: 0.9030\n",
      "Epoch 00366: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 367/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2702 - acc: 0.8770\n",
      "Epoch 00367: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 368/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2602 - acc: 0.8820\n",
      "Epoch 00368: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 369/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2473 - acc: 0.8930\n",
      "Epoch 00369: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 370/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2058 - acc: 0.9180\n",
      "Epoch 00370: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 371/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2831 - acc: 0.8780\n",
      "Epoch 00371: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 372/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2380 - acc: 0.9030\n",
      "Epoch 00372: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 373/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2433 - acc: 0.9070\n",
      "Epoch 00373: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 374/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2413 - acc: 0.9060\n",
      "Epoch 00374: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 375/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2573 - acc: 0.9000\n",
      "Epoch 00375: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 376/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2472 - acc: 0.9050\n",
      "Epoch 00376: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 377/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2454 - acc: 0.9070\n",
      "Epoch 00377: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 378/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2504 - acc: 0.8850\n",
      "Epoch 00378: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 379/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2595 - acc: 0.8930\n",
      "Epoch 00379: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 380/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2652 - acc: 0.8800\n",
      "Epoch 00380: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 381/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2339 - acc: 0.8920\n",
      "Epoch 00381: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 382/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2347 - acc: 0.9050\n",
      "Epoch 00382: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 383/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2566 - acc: 0.8760\n",
      "Epoch 00383: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 384/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2388 - acc: 0.8980\n",
      "Epoch 00384: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 385/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2328 - acc: 0.9090\n",
      "Epoch 00385: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 386/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2415 - acc: 0.9040\n",
      "Epoch 00386: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 387/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2366 - acc: 0.8990\n",
      "Epoch 00387: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 388/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2203 - acc: 0.9150\n",
      "Epoch 00388: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 2us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 389/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2719 - acc: 0.8910\n",
      "Epoch 00389: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 390/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2608 - acc: 0.8980\n",
      "Epoch 00390: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 391/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2532 - acc: 0.9080\n",
      "Epoch 00391: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 392/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2735 - acc: 0.8930\n",
      "Epoch 00392: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 393/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2612 - acc: 0.8950\n",
      "Epoch 00393: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 2us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 394/400\n",
      "15000/21450 [===================>..........] - ETA: 0s - loss: 0.2424 - acc: 0.8983\n",
      "Epoch 00394: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 395/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2612 - acc: 0.8840\n",
      "Epoch 00395: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 396/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2364 - acc: 0.8930\n",
      "Epoch 00396: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 397/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2465 - acc: 0.8980\n",
      "Epoch 00397: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 398/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2629 - acc: 0.8830\n",
      "Epoch 00398: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 2us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 399/400\n",
      "20000/21450 [==========================>...] - ETA: 0s - loss: 0.2485 - acc: 0.8965\n",
      "Epoch 00399: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n",
      "Epoch 400/400\n",
      " 1000/21450 [>.............................] - ETA: 0s - loss: 0.2396 - acc: 0.9100\n",
      "Epoch 00400: val_loss did not improve from 0.25358\n",
      "21450/21450 [==============================] - 0s 3us/sample - loss: 0.2478 - acc: 0.8968 - val_loss: 0.2536 - val_acc: 0.8978\n"
     ]
    }
   ],
   "source": [
    "bz = 1000\n",
    "epochs = 400\n",
    "history = model.fit(\n",
    "    xtrain,\n",
    "    ytrain,\n",
    "    epochs = epochs,\n",
    "    batch_size = bz,\n",
    "    validation_data =(xtest, ytest),\n",
    "    callbacks = [mcp, red_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7150/7150 [==============================] - 0s 18us/sample - loss: 0.2536 - acc: 0.8978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25357923249264697, 0.89776224]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss Vs # of epochs')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "plt.plot( [i for i in range(epochs)], history.history['val_loss'], label = \"validation loss\")\n",
    "plt.xlabel(\"#of epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Loss Vs # of epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x23b706db390>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(cos_d, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss Vs # of epochs')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(epochs)], history.history['loss'], label = \"Train loss\")\n",
    "plt.xlabel(\"#of epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Loss Vs # of epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Learning rate Vs # of epochs')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(epochs)], history.history['lr'], label = \"Train loss\")\n",
    "plt.xlabel(\"#of epochs\")\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.title(\"Learning rate Vs # of epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 15us/sample - loss: 0.2911 - acc: 0.8828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2910575838883718, 0.88283336]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc', 'lr'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best.load_weights(\"models_chpt/best_modelD.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 16us/sample - loss: 0.2911 - acc: 0.8828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2910575842857361, 0.88283336]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_best.evaluate(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision1",
   "language": "python",
   "name": "vision1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
